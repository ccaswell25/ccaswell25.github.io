[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "More details to come!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "More details to come!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Carly Caswell",
    "section": "",
    "text": "--- title: “Projects”\n---"
  },
  {
    "objectID": "posts/trial-python-render/STAC-search.html",
    "href": "posts/trial-python-render/STAC-search.html",
    "title": "ipynmb rendered as html",
    "section": "",
    "text": "Code\n#Import Libraries\nimport numpy as np \nimport geopandas as gpd\nimport rioxarray as rioxr\nimport matplotlib.pyplot as plt\n\nfrom shapely.geometry import Polygon\n\n#Used to access the STAC catalogs:\nfrom pystac_client import Client\n\n#Used to sign (requesting further access) items form the MPC STAC catalog:\nimport planetary_computer\n\n#Other libraries for nice outputs:\nfrom IPython.display import Image"
  },
  {
    "objectID": "posts/trial-python-render/STAC-search.html#access",
    "href": "posts/trial-python-render/STAC-search.html#access",
    "title": "ipynmb rendered as html",
    "section": "Access",
    "text": "Access\nWe use the Client function from the pystac_client package to access the catalog:\n\n\nCode\n#access catalog\ncatalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\", \n                      modifier = planetary_computer.sign_inplace)\n\n\nThe modifier parameter is needed to access the data in the MPC catalog."
  },
  {
    "objectID": "posts/trial-python-render/STAC-search.html#exploration",
    "href": "posts/trial-python-render/STAC-search.html#exploration",
    "title": "ipynmb rendered as html",
    "section": "Exploration",
    "text": "Exploration\nLet’s check out some of the catalog’s metadata:\n\n\nCode\n#metadata from the catalog:\nprint(\"Title:\", catalog.title)\nprint('Description:', catalog.description)\n\n\nTitle: Microsoft Planetary Computer STAC API\nDescription: Searchable spatiotemporal metadata describing Earth science datasets hosted by the Microsoft Planetary Computer\n\n\nWe can access the catalog’s collections by using the get_collections() method:\n\n\nCode\ncatalog.get_collections() #this doesn't tell us much\n\n\n&lt;generator object Client.get_collections at 0x7399c6603230&gt;\n\n\nNotice the output of get_collections() is a generator.\nThis is a special kind of lazy object in Python over which you can loop over like a list. Unlike a list, the items in the generator do not exist in memory until you explicitly iterate over them or convert them to a list.\nLet’s try getting the collections from the catalog again:\n\n\nCode\n# Get collections and print their names\ncollections = list(catalog.get_collections())\nprint('Number of collections:', len(collections))\nprint('Collections IDs:')\n      \nfor collection in collections:\n      print('-', collection.id)\n        \n# We can see the names of the collections that we just iterated over below:\n\n\nNumber of collections: 122\nCollections IDs:\n- daymet-annual-pr\n- daymet-daily-hi\n- 3dep-seamless\n- 3dep-lidar-dsm\n- fia\n- sentinel-1-rtc\n- gridmet\n- daymet-annual-na\n- daymet-monthly-na\n- daymet-annual-hi\n- daymet-monthly-hi\n- daymet-monthly-pr\n- gnatsgo-tables\n- hgb\n- cop-dem-glo-30\n- cop-dem-glo-90\n- goes-cmi\n- terraclimate\n- nasa-nex-gddp-cmip6\n- gpm-imerg-hhr\n- gnatsgo-rasters\n- 3dep-lidar-hag\n- 3dep-lidar-intensity\n- 3dep-lidar-pointsourceid\n- mtbs\n- noaa-c-cap\n- 3dep-lidar-copc\n- modis-64A1-061\n- alos-fnf-mosaic\n- 3dep-lidar-returns\n- mobi\n- landsat-c2-l2\n- era5-pds\n- chloris-biomass\n- kaza-hydroforecast\n- planet-nicfi-analytic\n- modis-17A2H-061\n- modis-11A2-061\n- daymet-daily-pr\n- 3dep-lidar-dtm-native\n- 3dep-lidar-classification\n- 3dep-lidar-dtm\n- gap\n- modis-17A2HGF-061\n- planet-nicfi-visual\n- gbif\n- modis-17A3HGF-061\n- modis-09A1-061\n- alos-dem\n- alos-palsar-mosaic\n- deltares-water-availability\n- modis-16A3GF-061\n- modis-21A2-061\n- us-census\n- jrc-gsw\n- deltares-floods\n- modis-43A4-061\n- modis-09Q1-061\n- modis-14A1-061\n- hrea\n- modis-13Q1-061\n- modis-14A2-061\n- sentinel-2-l2a\n- modis-15A2H-061\n- modis-11A1-061\n- modis-15A3H-061\n- modis-13A1-061\n- daymet-daily-na\n- nrcan-landcover\n- modis-10A2-061\n- ecmwf-forecast\n- noaa-mrms-qpe-24h-pass2\n- sentinel-1-grd\n- nasadem\n- io-lulc\n- landsat-c2-l1\n- drcog-lulc\n- chesapeake-lc-7\n- chesapeake-lc-13\n- chesapeake-lu\n- noaa-mrms-qpe-1h-pass1\n- noaa-mrms-qpe-1h-pass2\n- noaa-nclimgrid-monthly\n- goes-glm\n- usda-cdl\n- eclipse\n- esa-cci-lc\n- esa-cci-lc-netcdf\n- fws-nwi\n- usgs-lcmap-conus-v13\n- usgs-lcmap-hawaii-v10\n- noaa-climate-normals-tabular\n- noaa-climate-normals-netcdf\n- noaa-climate-normals-gridded\n- aster-l1t\n- cil-gdpcir-cc-by-sa\n- io-lulc-9-class\n- io-biodiversity\n- naip\n- noaa-cdr-sea-surface-temperature-whoi\n- noaa-cdr-ocean-heat-content\n- cil-gdpcir-cc0\n- cil-gdpcir-cc-by\n- noaa-cdr-sea-surface-temperature-whoi-netcdf\n- noaa-cdr-sea-surface-temperature-optimum-interpolation\n- modis-10A1-061\n- sentinel-5p-l2-netcdf\n- sentinel-3-olci-wfr-l2-netcdf\n- noaa-cdr-ocean-heat-content-netcdf\n- sentinel-3-synergy-aod-l2-netcdf\n- sentinel-3-synergy-v10-l2-netcdf\n- sentinel-3-olci-lfr-l2-netcdf\n- sentinel-3-sral-lan-l2-netcdf\n- sentinel-3-slstr-lst-l2-netcdf\n- sentinel-3-slstr-wst-l2-netcdf\n- sentinel-3-sral-wat-l2-netcdf\n- ms-buildings\n- sentinel-3-slstr-frp-l2-netcdf\n- sentinel-3-synergy-syn-l2-netcdf\n- sentinel-3-synergy-vgp-l2-netcdf\n- sentinel-3-synergy-vg1-l2-netcdf\n- esa-worldcover"
  },
  {
    "objectID": "posts/trial-python-render/STAC-search.html#collection",
    "href": "posts/trial-python-render/STAC-search.html#collection",
    "title": "ipynmb rendered as html",
    "section": "Collection",
    "text": "Collection\nWe can select a single collection for exploration using the get_child() method for the catalog and the collection id as the parameter:\n\n\nCode\nnaip_collection = catalog.get_child('naip')\nnaip_collection\n\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    CollectionClient: naip\n                \n            \n            \n\n\n\n\n\n\nid: naip\n\n\ntitle: NAIP: National Agriculture Imagery Program\n\n\ndescription: The [National Agriculture Imagery Program](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) (NAIP) provides U.S.-wide, high-resolution aerial imagery, with four spectral bands (R, G, B, IR). NAIP is administered by the [Aerial Field Photography Office](https://www.fsa.usda.gov/programs-and-services/aerial-photography/) (AFPO) within the [US Department of Agriculture](https://www.usda.gov/) (USDA). Data are captured at least once every three years for each state. This dataset represents NAIP data from 2010-present, in [cloud-optimized GeoTIFF](https://www.cogeo.org/) format.\n\n\nproviders:\n\n\nUSDA Farm Service Agency (producer, licensor)\n\n\nEsri (processor)\n\n\nMicrosoft (host, processor)\n\n\n\n\ntype: Collection\n\n\nitem_assets: {'image': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized', 'roles': ['data'], 'title': 'RGBIR COG tile', 'eo:bands': [{'name': 'Red', 'common_name': 'red'}, {'name': 'Green', 'common_name': 'green'}, {'name': 'Blue', 'common_name': 'blue'}, {'name': 'NIR', 'common_name': 'nir', 'description': 'near-infrared'}]}, 'metadata': {'type': 'text/plain', 'roles': ['metadata'], 'title': 'FGDC Metdata'}, 'thumbnail': {'type': 'image/jpeg', 'roles': ['thumbnail'], 'title': 'Thumbnail'}}\n\n\nmsft:region: westeurope\n\n\nmsft:container: naip\n\n\nmsft:storage_account: naipeuwest\n\n\nmsft:short_description: NAIP provides US-wide, high-resolution aerial imagery. This dataset includes NAIP images from 2010 to the present.\n\n\n\n\nSTAC Extensions\n\n\n\nhttps://stac-extensions.github.io/item-assets/v1.0.0/schema.json\n\n\nhttps://stac-extensions.github.io/table/v1.2.0/schema.json\n\n\n\n\n\nItems\nOnly the first item shown\n\n\n\n\n\nItem: hi_m_2015561_sw_05_060_20211226_20220909\n\n\n\nid: hi_m_2015561_sw_05_060_20211226_20220909\n\n\nbbox: [-155.502923, 19.997278, -155.434587, 20.065225]\n\n\ngsd: 0.6\n\n\ndatetime: 2021-12-26T16:00:00Z\n\n\nnaip:year: 2021\n\n\nproj:bbox: [238224.0, 2213136.0, 245268.0, 2220558.0]\n\n\nproj:epsg: 26905\n\n\nproviders: [{'url': 'https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/', 'name': 'USDA Farm Service Agency', 'roles': ['producer', 'licensor']}]\n\n\nnaip:state: hi\n\n\nproj:shape: [12370, 11740]\n\n\nproj:transform: [0.6, 0.0, 238224.0, 0.0, -0.6, 2220558.0, 0.0, 0.0, 1.0]\n\n\n\n\nSTAC Extensions\n\n\n\nhttps://stac-extensions.github.io/eo/v1.0.0/schema.json\n\n\nhttps://stac-extensions.github.io/projection/v1.0.0/schema.json\n\n\n\n\n\nAssets\n\n\n\n\n\nAsset: RGBIR COG tile\n\n\n\nhref: https://naipeuwest.blob.core.windows.net/naip/v002/hi/2021/hi_060cm_2021/20155/61/m_2015561_sw_05_060_20211226_20220909.tif\n\n\ntype: image/tiff; application=geotiff; profile=cloud-optimized\n\n\ntitle: RGBIR COG tile\n\n\nroles: ['data']\n\n\nowner: hi_m_2015561_sw_05_060_20211226_20220909\n\n\neo:bands: [{'name': 'Red', 'common_name': 'red'}, {'name': 'Green', 'common_name': 'green'}, {'name': 'Blue', 'common_name': 'blue'}, {'name': 'NIR', 'common_name': 'nir', 'description': 'near-infrared'}]\n\n\n\n\n\n\n\n\n\n\nAsset: Thumbnail\n\n\n\nhref: https://naipeuwest.blob.core.windows.net/naip/v002/hi/2021/hi_060cm_2021/20155/m_2015561_sw_05_060_20211226_20220909.200.jpg\n\n\ntype: image/jpeg\n\n\ntitle: Thumbnail\n\n\nroles: ['thumbnail']\n\n\nowner: hi_m_2015561_sw_05_060_20211226_20220909\n\n\n\n\n\n\n\n\n\n\nAsset: TileJSON with default rendering\n\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/tilejson.json?collection=naip&item=hi_m_2015561_sw_05_060_20211226_20220909&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png\n\n\ntype: application/json\n\n\ntitle: TileJSON with default rendering\n\n\nroles: ['tiles']\n\n\nowner: hi_m_2015561_sw_05_060_20211226_20220909\n\n\n\n\n\n\n\n\n\n\nAsset: Rendered preview\n\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/preview.png?collection=naip&item=hi_m_2015561_sw_05_060_20211226_20220909&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png\n\n\ntype: image/png\n\n\ntitle: Rendered preview\n\n\nroles: ['overview']\n\n\nowner: hi_m_2015561_sw_05_060_20211226_20220909\n\n\nrel: preview\n\n\n\n\n\n\n\nLinks\n\n\n\n\n\nLink:\n\n\n\nrel: collection\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip\n\n\ntype: application/json\n\n\n\n\n\n\n\n\n\n\nLink:\n\n\n\nrel: parent\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip\n\n\ntype: application/json\n\n\n\n\n\n\n\n\n\n\nLink: Microsoft Planetary Computer STAC API\n\n\n\nrel: root\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/\n\n\ntype: application/json\n\n\ntitle: Microsoft Planetary Computer STAC API\n\n\n\n\n\n\n\n\n\n\nLink:\n\n\n\nrel: self\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip/items/hi_m_2015561_sw_05_060_20211226_20220909\n\n\ntype: application/geo+json\n\n\n\n\n\n\n\n\n\n\nLink: Map of item\n\n\n\nrel: preview\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/map?collection=naip&item=hi_m_2015561_sw_05_060_20211226_20220909\n\n\ntype: text/html\n\n\ntitle: Map of item\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\n\nLink:\n\n\n\nrel: items\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip/items\n\n\ntype: application/geo+json\n\n\n\n\n\n\n\n\n\n\nLink: Microsoft Planetary Computer STAC API\n\n\n\nrel: root\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/\n\n\ntype: application/json\n\n\ntitle: Microsoft Planetary Computer STAC API\n\n\n\n\n\n\n\n\n\n\nLink: Public Domain\n\n\n\nrel: license\n\n\nhref: https://www.fsa.usda.gov/help/policies-and-links/\n\n\ntitle: Public Domain\n\n\n\n\n\n\n\n\n\n\nLink: Human readable dataset overview and reference\n\n\n\nrel: describedby\n\n\nhref: https://planetarycomputer.microsoft.com/dataset/naip\n\n\ntype: text/html\n\n\ntitle: Human readable dataset overview and reference\n\n\n\n\n\n\n\n\n\n\nLink:\n\n\n\nrel: self\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip\n\n\ntype: application/json\n\n\n\n\n\n\n\n\n\n\nLink: Microsoft Planetary Computer STAC API\n\n\n\nrel: parent\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1\n\n\ntype: application/json\n\n\ntitle: Microsoft Planetary Computer STAC API\n\n\n\n\n\n\n\nAssets\n\n\n\n\n\nAsset: NAIP thumbnail\n\n\n\nhref: https://ai4edatasetspublicassets.blob.core.windows.net/assets/pc_thumbnails/naip.png\n\n\ntype: image/png\n\n\ntitle: NAIP thumbnail\n\n\nroles: ['thumbnail']\n\n\nowner: naip\n\n\n\n\n\n\n\n\n\n\nAsset: GeoParquet STAC items\n\n\n\nhref: abfs://items/naip.parquet\n\n\ntype: application/x-parquet\n\n\ntitle: GeoParquet STAC items\n\n\ndescription: Snapshot of the collection's STAC items exported to GeoParquet format.\n\n\nroles: ['stac-items']\n\n\nowner: naip\n\n\nmsft:partition_info: {'is_partitioned': True, 'partition_frequency': 'AS'}\n\n\ntable:storage_options: {'account_name': 'pcstacitems'}"
  },
  {
    "objectID": "posts/trial-python-render/STAC-search.html#catalog-search",
    "href": "posts/trial-python-render/STAC-search.html#catalog-search",
    "title": "ipynmb rendered as html",
    "section": "Catalog Search",
    "text": "Catalog Search\nWe can narrow the search within the catalog by specifying a time range, and area of interest, and the collection name.\nThe simples way to define the area of interest to look for in the catalog are: - a GeoJSON-type dictionary with coordinates of the bounding box - as a list [xmin, ymin, xmax, ymax] with the coordinate values defining the four corners of the bounding box\nYou could also use a point, or some more complex polygon.\nIn this lesson we will look for NAIP scenes over Santa Barbara from 2018 to 2023. We’ll use the GeoJSON method to define the area of interest:\n\n\nCode\n#temporal range of interest\ntime_range = \"2018-01-01/2023-01-01\"\n\n#NCEAS bounding box (as a GeoJSON)\nbbox = {\n    \"type\": \"Polygon\",\n    \"coordinates\":[\n        [\n            [-119.70608227128903, 34.426300194372274],\n            [-119.70608227128903, 34.42041139020533],\n            [-119.6967885126002, 34.42041139020533],\n            [-119.6967885126002, 34.426300194372274],\n            [-119.70608227128903, 34.426300194372274]\n        ]\n    ],\n}\n\n#Catalog search \nsearch = catalog.search(\n    collections = ['naip'], # a list of collection id\n    intersects = bbox,\n    datetime = time_range\n)\n\nsearch\n\n\n&lt;pystac_client.item_search.ItemSearch at 0x7399c5f3db90&gt;\n\n\nTo get the items found in the search (or check if there were any matches in the search) we use the item_collection() method:\n\n\nCode\nitems = search.item_collection()\n\n#Number of items in the search:\nlen(items)\n\n\n2\n\n\n\n\nCode\n#Take a look at the items:\nitems"
  },
  {
    "objectID": "posts/trial-python-render/STAC-search.html#items",
    "href": "posts/trial-python-render/STAC-search.html#items",
    "title": "ipynmb rendered as html",
    "section": "Items",
    "text": "Items\nLet’s get the first item in the search\n\n\nCode\n#get first item in teh catalog search\nitem = items[0]\ntype(item)\n\n\npystac.item.Item\n\n\nRemember the STAC item is the cored object in the catalog.\nThe item does not contain the data itself, but rather the metadata about it and links the access to actual data (assets). Some of the metadata:\n\n\nCode\nprint('id', item.id)\nitem.properties\n\n\nid ca_m_3411935_sw_11_060_20200521\n\n\n{'gsd': 0.6,\n 'datetime': '2020-05-21T00:00:00Z',\n 'naip:year': '2020',\n 'proj:bbox': [246930.0, 3806808.0, 253260.0, 3814296.0],\n 'proj:epsg': 26911,\n 'naip:state': 'ca',\n 'proj:shape': [12480, 10550],\n 'proj:transform': [0.6, 0.0, 246930.0, 0.0, -0.6, 3814296.0, 0.0, 0.0, 1.0]}\n\n\nJust as the item properties, the item assets are given in a dictionary, with each value being a pystac.asset. Let’s check the assets in the item:\n\n\nCode\nitem.assets #the assets are links to the actual data \n\n\n{'image': &lt;Asset href=https://naipeuwest.blob.core.windows.net/naip/v002/ca/2020/ca_060cm_2020/34119/m_3411935_sw_11_060_20200521.tif?st=2023-11-26T21%3A20%3A06Z&se=2023-12-04T21%3A20%3A07Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-27T21%3A20%3A05Z&ske=2023-12-04T21%3A20%3A05Z&sks=b&skv=2021-06-08&sig=NEtxfBvrqXrHQD3IKraClK//oXTCKQrqIVkXYiiXGJw%3D&gt;,\n 'thumbnail': &lt;Asset href=https://naipeuwest.blob.core.windows.net/naip/v002/ca/2020/ca_060cm_2020/34119/m_3411935_sw_11_060_20200521.200.jpg?st=2023-11-26T21%3A20%3A06Z&se=2023-12-04T21%3A20%3A07Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-27T21%3A20%3A05Z&ske=2023-12-04T21%3A20%3A05Z&sks=b&skv=2021-06-08&sig=NEtxfBvrqXrHQD3IKraClK//oXTCKQrqIVkXYiiXGJw%3D&gt;,\n 'tilejson': &lt;Asset href=https://planetarycomputer.microsoft.com/api/data/v1/item/tilejson.json?collection=naip&item=ca_m_3411935_sw_11_060_20200521&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png&gt;,\n 'rendered_preview': &lt;Asset href=https://planetarycomputer.microsoft.com/api/data/v1/item/preview.png?collection=naip&item=ca_m_3411935_sw_11_060_20200521&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png&gt;}\n\n\n\n\nCode\nfor key in item.assets.keys():\n    print(key, '--', item.assets[key].title)\n    \n#for every key in the assets, print the key name and access the title of the assets\n\n\nimage -- RGBIR COG tile\nthumbnail -- Thumbnail\ntilejson -- TileJSON with default rendering\nrendered_preview -- Rendered preview\n\n\n\n\nCode\nitem.assets.keys() #these are the keys from our assets dictionary\n\n\ndict_keys(['image', 'thumbnail', 'tilejson', 'rendered_preview'])\n\n\nNotice each asset has an href, which is a link to the asset object(ie. the data). For example, we can use the URL for the rendered preview asset to plot it:\n\n\nCode\nImage(url=item.assets['rendered_preview'].href, width = 500) #retrieving image and showing the output"
  },
  {
    "objectID": "posts/trial-python-render/STAC-search.html#load-data",
    "href": "posts/trial-python-render/STAC-search.html#load-data",
    "title": "ipynmb rendered as html",
    "section": "Load Data",
    "text": "Load Data\nThe raster data in our current item is in the image asset. Again, we access this data via its URL.\nThis time we open it using rioxr.open_rasterio() directly:\n\n\nCode\nsb = rioxr.open_rasterio(item.assets['image'].href)\nsb\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 4, y: 12480, x: 10550)&gt;\n[526656000 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3 4\n  * x            (x) float64 2.469e+05 2.469e+05 ... 2.533e+05 2.533e+05\n  * y            (y) float64 3.814e+06 3.814e+06 ... 3.807e+06 3.807e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:             Area\n    TIFFTAG_IMAGEDESCRIPTION:  OrthoVista\n    TIFFTAG_RESOLUTIONUNIT:    1 (unitless)\n    TIFFTAG_SOFTWARE:          Trimble Germany GmbH\n    TIFFTAG_XRESOLUTION:       1\n    TIFFTAG_YRESOLUTION:       1\n    _FillValue:                0\n    scale_factor:              1.0\n    add_offset:                0.0xarray.DataArrayband: 4y: 12480x: 10550...[526656000 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3 4array([1, 2, 3, 4])x(x)float642.469e+05 2.469e+05 ... 2.533e+05array([246930.3, 246930.9, 246931.5, ..., 253258.5, 253259.1, 253259.7])y(y)float643.814e+06 3.814e+06 ... 3.807e+06array([3814295.7, 3814295.1, 3814294.5, ..., 3806809.5, 3806808.9, 3806808.3])spatial_ref()int640crs_wkt :PROJCS[\"NAD83 / UTM zone 11N\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"26911\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314140356inverse_flattening :298.257222101reference_ellipsoid_name :GRS 1980longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :NAD83horizontal_datum_name :North American Datum 1983projected_crs_name :NAD83 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"NAD83 / UTM zone 11N\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"26911\"]]GeoTransform :246930.0 0.6 0.0 3814296.0 0.0 -0.6array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1, 2, 3, 4], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([          246930.3,           246930.9,           246931.5,\n       246932.09999999998, 246932.69999999998,           246933.3,\n                 246933.9,           246934.5, 246935.09999999998,\n       246935.69999999998,\n       ...\n                 253254.3,           253254.9,           253255.5,\n       253256.09999999998, 253256.69999999998,           253257.3,\n                 253257.9,           253258.5, 253259.09999999998,\n       253259.69999999998],\n      dtype='float64', name='x', length=10550))yPandasIndexPandasIndex(Index([         3814295.7,          3814295.1,          3814294.5,\n       3814293.9000000004, 3814293.3000000003,          3814292.7,\n                3814292.1,          3814291.5, 3814290.9000000004,\n       3814290.3000000003,\n       ...\n                3806813.7,          3806813.1,          3806812.5,\n       3806811.9000000004, 3806811.3000000003,          3806810.7,\n                3806810.1,          3806809.5, 3806808.9000000004,\n       3806808.3000000003],\n      dtype='float64', name='y', length=12480))Attributes: (9)AREA_OR_POINT :AreaTIFFTAG_IMAGEDESCRIPTION :OrthoVistaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_SOFTWARE :Trimble Germany GmbHTIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :0scale_factor :1.0add_offset :0.0\n\n\n\n\nCode\n# Plot raster with correct ratio\n\nsize = 6 #height in of plot \naspect = sb.rio.width/sb.rio.height\nsb.sel(band=[1,2,3]).plot.imshow(size=size, aspect=aspect)\n\n\n&lt;matplotlib.image.AxesImage at 0x7399bfec8a10&gt;"
  },
  {
    "objectID": "posts/trial-python-render/STAC-search.html#exercise",
    "href": "posts/trial-python-render/STAC-search.html#exercise",
    "title": "ipynmb rendered as html",
    "section": "Exercise",
    "text": "Exercise\nThe cop-dem-glo-90 (id of collection) collection contains the Copernicus DEM at 90m resolution (the one we used for the Grand Canyon).\n\nUse the bbox for Santa Barbara to look for items in this collection\nGet the first item in the search and check its assets\nPlot the item’s rendered preview asset\nOpen the item’s data using rioxarray\n\n\n\nCode\n#Use the bbox for Santa Barbara for items in collection: cop-demo-glo90\nsearch2 = catalog.search(\n    collections = ['cop-dem-glo-90'], # a list of collection id\n    intersects = bbox,\n    datetime = time_range\n)\n\nitems2 = search2.item_collection()\n\n#Number of items in the search:\nlen(items2)\n\nitems2\n\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    ItemCollection\n                \n            \n            \n\nItems\n\n\n\n\n\nItem: Copernicus_DSM_COG_30_N34_00_W120_00_DEM\n\n\n\nid: Copernicus_DSM_COG_30_N34_00_W120_00_DEM\n\n\nbbox: [-120.00041666666667, 34.000416666666666, -119.00041666666667, 35.000416666666666]\n\n\ngsd: 90\n\n\ndatetime: 2021-04-22T00:00:00Z\n\n\nplatform: TanDEM-X\n\n\nproj:epsg: 4326\n\n\nproj:shape: [1200, 1200]\n\n\nproj:transform: [0.0008333333333333334, 0.0, -120.00041666666667, 0.0, -0.0008333333333333334, 35.000416666666666]\n\n\n\n\nSTAC Extensions\n\n\n\nhttps://stac-extensions.github.io/projection/v1.0.0/schema.json\n\n\n\n\n\nAssets\n\n\n\n\n\nAsset: N34_00_W120_00\n\n\n\nhref: https://elevationeuwest.blob.core.windows.net/copernicus-dem/COP90_hh/Copernicus_DSM_COG_30_N34_00_W120_00_DEM.tif?st=2023-11-26T21%3A39%3A40Z&se=2023-12-04T21%3A39%3A42Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-27T21%3A39%3A39Z&ske=2023-12-04T21%3A39%3A39Z&sks=b&skv=2021-06-08&sig=qt4yGbZ/9krZE9KfP0/RWBPymTc8UKhAficlBHi%2B95g%3D\n\n\ntype: image/tiff; application=geotiff; profile=cloud-optimized\n\n\ntitle: N34_00_W120_00\n\n\nroles: ['data']\n\n\nowner: Copernicus_DSM_COG_30_N34_00_W120_00_DEM\n\n\n\n\n\n\n\n\n\n\nAsset: TileJSON with default rendering\n\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/tilejson.json?collection=cop-dem-glo-90&item=Copernicus_DSM_COG_30_N34_00_W120_00_DEM&assets=data&colormap_name=terrain&rescale=-1000%2C4000&format=png\n\n\ntype: application/json\n\n\ntitle: TileJSON with default rendering\n\n\nroles: ['tiles']\n\n\nowner: Copernicus_DSM_COG_30_N34_00_W120_00_DEM\n\n\n\n\n\n\n\n\n\n\nAsset: Rendered preview\n\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/preview.png?collection=cop-dem-glo-90&item=Copernicus_DSM_COG_30_N34_00_W120_00_DEM&assets=data&colormap_name=terrain&rescale=-1000%2C4000&format=png\n\n\ntype: image/png\n\n\ntitle: Rendered preview\n\n\nroles: ['overview']\n\n\nowner: Copernicus_DSM_COG_30_N34_00_W120_00_DEM\n\n\nrel: preview\n\n\n\n\n\n\n\nLinks\n\n\n\n\n\nLink:\n\n\n\nrel: collection\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/cop-dem-glo-90\n\n\ntype: application/json\n\n\n\n\n\n\n\n\n\n\nLink:\n\n\n\nrel: parent\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/cop-dem-glo-90\n\n\ntype: application/json\n\n\n\n\n\n\n\n\n\n\nLink: Microsoft Planetary Computer STAC API\n\n\n\nrel: root\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1\n\n\ntype: application/json\n\n\ntitle: Microsoft Planetary Computer STAC API\n\n\n\n\n\n\n\n\n\n\nLink:\n\n\n\nrel: self\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/cop-dem-glo-90/items/Copernicus_DSM_COG_30_N34_00_W120_00_DEM\n\n\ntype: application/geo+json\n\n\n\n\n\n\n\n\n\n\nLink: Copernicus DEM User handbook\n\n\n\nrel: handbook\n\n\nhref: https://object.cloud.sdsc.edu/v1/AUTH_opentopography/www/metadata/Copernicus_metadata.pdf\n\n\ntype: application/pdf\n\n\ntitle: Copernicus DEM User handbook\n\n\n\n\n\n\n\n\n\n\nLink: Map of item\n\n\n\nrel: preview\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/map?collection=cop-dem-glo-90&item=Copernicus_DSM_COG_30_N34_00_W120_00_DEM\n\n\ntype: text/html\n\n\ntitle: Map of item"
  },
  {
    "objectID": "posts/2023-10-29-short-post-description/index.html",
    "href": "posts/2023-10-29-short-post-description/index.html",
    "title": "test blog post title",
    "section": "",
    "text": "I am going to insert a footnote here1."
  },
  {
    "objectID": "posts/2023-10-29-short-post-description/index.html#footnotes",
    "href": "posts/2023-10-29-short-post-description/index.html#footnotes",
    "title": "test blog post title",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is the associated footnote, which will appear at the bottom of my document in a “Footnotes” section.↩︎"
  },
  {
    "objectID": "posts/trial-python-render/eds242-finalblog.html",
    "href": "posts/trial-python-render/eds242-finalblog.html",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "",
    "text": "Artificial Intelligence (AI) stands out as a prevailing buzzword of this era. It is everywhere we live, work, and interact - from the advertisements on our screens to the technology in our homes and in integral services like those provided by banks, airports, and hospitals. At the basis of AI is algorithms, a systematic set of instructions or rules used to solve problems. Focusing on efforts related to climate work, algorithm application can be applied to predicting temperature changes, weather events, future deforestation, and carbon emissions. It can be used to show the effects of extreme weather, potential benefits of carbon capture and regenerative agriculture, and even nudge the general public to pursue climate-friendly ways of changing habits and behaviors (Coeckelbergh).\nAlgorithms, however, can lead to biased decision-making. There is plenty of evidence that exists to prove biases in algorithms, including research on algorithms detecting skin cancer that was only effective on light skin tones because of a non-demographically diverse dataset (Calderon) or Amazon’s hiring algorithm exhibiting gender bias when designed to review job applicants, favoring male candidates over female candidates because the training data reflected a male-dominated workforce (Dastin). Algorithmic-generated content needs to be critiqued and reviewed through the lens of auditing. Algorithmic auditing is a crucial way to address the challenges associated with the increasing use of algorithms and dependence of them on our decision-making. In order to audit algorithms effectively, we need to question the basis of AI before allowing it to drive our human-based decisions (whether we consider AI to be “moral” decision-making is a whole other topic that I won’t get into today). When considering environmentally-focused algorithmic decision-making, I think it’s crucial to contemplate three aspects of algorithm creation: Data, Documentation, and Decision-makers (3Ds)."
  },
  {
    "objectID": "posts/trial-python-render/eds242-finalblog.html#the-three-ds-analyzing-the-implications-of-data-documentation-and-decision-making-in-climate-driven-artificial-intelligence",
    "href": "posts/trial-python-render/eds242-finalblog.html#the-three-ds-analyzing-the-implications-of-data-documentation-and-decision-making-in-climate-driven-artificial-intelligence",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "",
    "text": "Artificial Intelligence (AI) stands out as a prevailing buzzword of this era. It is everywhere we live, work, and interact - from the advertisements on our screens to the technology in our homes and in integral services like those provided by banks, airports, and hospitals. At the basis of AI is algorithms, a systematic set of instructions or rules used to solve problems. Focusing on efforts related to climate work, algorithm application can be applied to predicting temperature changes, weather events, future deforestation, and carbon emissions. It can be used to show the effects of extreme weather, potential benefits of carbon capture and regenerative agriculture, and even nudge the general public to pursue climate-friendly ways of changing habits and behaviors (Coeckelbergh).\nAlgorithms, however, can lead to biased decision-making. There is plenty of evidence that exists to prove biases in algorithms, including research on algorithms detecting skin cancer that was only effective on light skin tones because of a non-demographically diverse dataset (Calderon) or Amazon’s hiring algorithm exhibiting gender bias when designed to review job applicants, favoring male candidates over female candidates because the training data reflected a male-dominated workforce (Dastin). Algorithmic-generated content needs to be critiqued and reviewed through the lens of auditing. Algorithmic auditing is a crucial way to address the challenges associated with the increasing use of algorithms and dependence of them on our decision-making. In order to audit algorithms effectively, we need to question the basis of AI before allowing it to drive our human-based decisions (whether we consider AI to be “moral” decision-making is a whole other topic that I won’t get into today). When considering environmentally-focused algorithmic decision-making, I think it’s crucial to contemplate three aspects of algorithm creation: Data, Documentation, and Decision-makers (3Ds)."
  },
  {
    "objectID": "posts/trial-python-render/eds242-finalblog.html#data-weve-got-a-problem",
    "href": "posts/trial-python-render/eds242-finalblog.html#data-weve-got-a-problem",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Data, We’ve Got a Problem",
    "text": "Data, We’ve Got a Problem\nAs a Data Scientist, I interact with new and evolving datasets just about every day. This data is sourced from a variety of topics and contributors, and in the work that I do, I’ve been taught to take a deeper look, ask questions, and consider the biases that went into the data. According to Jennifer Logg, an Assistant Professor of Management at Georgetown University, “….algorithms can efficiently compound bias that is present in the input data. An algorithm will magnify any patterns in the input data, so if bias is present, the algorithm will also magnify that bias, “(Rock). Climate justice relies heavily on accurate data representation to inform policies and decision-making processes. When biased data becomes the inputs to algorithms that shape these decisions, it can result in disproportionate impacts on marginalized communities. If these biases are not properly addressed during algorithmic analysis, it can lead to the reinforcement of existing disparities. For example, an algorithm that relies on biased pollution data might misallocate resources in its decision-making, leaving already vulnerable communities without protection. To achieve environmental justice in AI-driven climate work, we need to acknowledge, scrutinize, and correct biases in the data used by algorithms. By prioritizing fairness in data analysis, we can build algorithms that contribute to equitable environmental decisions, policies, and practices."
  },
  {
    "objectID": "posts/trial-python-render/eds242-finalblog.html#document-document-document",
    "href": "posts/trial-python-render/eds242-finalblog.html#document-document-document",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Document! Document! Document!",
    "text": "Document! Document! Document!\nSimilar to the creation of datasheets for datasets, algorithm-driven decisions need to have documentation formulating the inner workings of the algorithm, including its design, logic, and decision-making processes. This transparency can allow auditors and stakeholders to understand how the algorithm operates, which is essential for assessing its fairness, accuracy, and potential biases. I believe documentation in the form of some sort of watermarking system, could allow users to have transparency and future trust in the algorithms they are interacting with. Thorough documentation, in general, contributes to better transparency, reproducibility, and accountability of algorithmic systems, making the auditing process more effective and reliable. With watermarking systems, we can ensure that auditors have “checked” off the necessary information for assessing an algorithm’s performance, and if unable to watermark for approval, could identify potential issues, and make informed recommendations for improvement."
  },
  {
    "objectID": "posts/trial-python-render/eds242-finalblog.html#who-decides",
    "href": "posts/trial-python-render/eds242-finalblog.html#who-decides",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Who Decides?",
    "text": "Who Decides?\nEffective auditing (as well as creation) of algorithms is reliant on informed decision-makers who understand the nuances of ethical data and decision-making. As Coecklebergh states in AI for Climate, “….those who develop and use AI have a special (in the sense of “specific”) responsibility. To make sure that AI leads to a greener and more climate friendly world is definitely also the responsibility of computer scientists, engineers, designers, managers, investors, and others involved in, managing, and promoting, AI and data science practices,” (Coeckelbergh). We also need to bring to the table decision-makers that are going to make fair decisions and keep the general public in mind. Consider Google’s recent creation of their Advanced Technology Ethics Advisory council, which aimed to advise on the company’s usage of AI. “….they were not transparent about their roles, responsibilities, and authority. Rather than engage affected communities, Google appointed a Council member who opposed LGBT rights. Google’s approach to oversight fostered distrust and protests, and the Council was dissolved,” (Calderon). In my opinion, human intervention will always be needed to create checks and balances with any form of AI that is driving our decisions, behaviors, and analyses. In the realm of climate justice, where algorithmic systems can impact something like policy formation, knowledgeable, fair, and adequately represented decision-makers are crucial to the formation and usage of AI in climate work."
  },
  {
    "objectID": "posts/trial-python-render/eds242-finalblog.html#ai-dont-worry-we-still-love-you",
    "href": "posts/trial-python-render/eds242-finalblog.html#ai-dont-worry-we-still-love-you",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "AI, Don’t Worry We Still Love You",
    "text": "AI, Don’t Worry We Still Love You\nTo conclude, in order to promote ethical data usage and responsible AI application we must safeguard future use of AI in environmental justice by taking a fine-tooth comb to the 3Ds (Data, Documentation, and Decision-makers). As Jennifer Logg so eloquently stated, “Trashing the mirror does not heal the bruise, but it could prolong the time it takes to fix the problem and detect future ones,”(Rock). In an era where algorithms increasingly influence critical aspects of our lives, of indigenous communities, of nature, and of our planet, we need to ensure these rapidly emerging systems undergo rigorous scrutiny and auditing. If we implement the three D’s to algorithm creation and usage, and even consider a “stamp of approval”, we can have some level of a “digital signature”, attesting to the legitimacy and ethical compliance of the underlying processes. This stamp of approval could then be implemented in policies and compliance for future creation and usage of AI in not just climate application, but any field."
  },
  {
    "objectID": "posts/trial-python-render/eds242-finalblog.html#sources",
    "href": "posts/trial-python-render/eds242-finalblog.html#sources",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Sources:",
    "text": "Sources:\nCalderon, A., Taber, D., Qu, H., Wen, J., et al. (2019). AI Blindspot Cards. Retrieved from www.aiblindspot.com (Version 1.1).\nCoeckelbergh, M. (2020). “AI for climate: freedom, justice, and other ethical and political challenges.” AI and Ethics, Pg 1-6.\nDastin, J. (2018, October 10). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. Retrieved from https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G\nRock, D., Whittlestone, J., & Garrett, N. (2019, August 7). Using Algorithms to Understand the Biases in Your Organization. Harvard Business Review. Retrieved from https://hbr.org/2019/08/using-algorithms-to-understand-the-biases-in-your-organization\n​​Smith, J. (2022, March 15). How AI Can Help Tackle Climate Change. Techopedia, https://www.techopedia.com/how-ai-can-help-tackle-climate-change/2/33622"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carly Caswell",
    "section": "",
    "text": "My name is Carly and welcome to my personal website. This site will give you an overview of my professional life, interests, and passions (both personal and work-related).\nWhat am I up to right now?\nI am currently a Master of Environmental Data Science student at the University of California Santa Barbara, studying to apply data science skills to real challenges facing our ever-changing climate.\n\n\nMaster in Environmental Data Science [Present] University of California, Santa Barbara\nBS in Business Administration [2018] University of Vermont\n\n\n\nEntrepreneurship Teaching Assistant [2023 - Present] UCSB\nSolutions Consultant [2021 - 2023] Smartsheet\nAI Solutions Consultant [2018 - 2021] Quinyx"
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Carly Caswell",
    "section": "",
    "text": "My name is Carly and welcome to my personal website. This site will give you an overview of my professional life, interests, and passions (both personal and work-related).\nWhat am I up to right now?\nI am currently a Master of Environmental Data Science student at the University of California Santa Barbara, studying to apply data science skills to real challenges facing our ever-changing climate.\n\n\nMaster in Environmental Data Science [Present] University of California, Santa Barbara\nBS in Business Administration [2018] University of Vermont\n\n\n\nEntrepreneurship Teaching Assistant [2023 - Present] UCSB\nSolutions Consultant [2021 - 2023] Smartsheet\nAI Solutions Consultant [2018 - 2021] Quinyx"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "ipynmb rendered as html\n\n\ntest description\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence\n\n\n\nAI\n\n\nEthics\n\n\nClimate\n\n\nMEDS\n\n\n\nA blog for EDS242: Ethics and Bias in Data Science\n\n\n\nCarly Caswell\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest blog post title\n\n\n\nQuarto\n\n\nR\n\n\nMEDS\n\n\n\nblog post test description\n\n\n\nCarly Caswell\n\n\nOct 29, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]
[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Projects & Blog",
    "section": "",
    "text": "Utilizing Strava Data to Evalute My Marathon Training\n\n\n\nStrava\n\n\nR\n\n\nData Visualization\n\n\nMEDS\n\n\n\nAn infographic analyzing and visualizing my Strava data when marathon training in 2023.\n\n\n\nCarly Caswell\n\n\nMar 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Suitable Growth Areas for Marine Species on the West Coast of the United States\n\n\n\nGIS\n\n\nR\n\n\nMEDS\n\n\n\nDetermining growth suitability for developing marine aquaculture on the West Coast the United States Exclusive Economic Zones (EEZ).\n\n\n\nCarly Caswell\n\n\nDec 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating the Impact of Climate Change on Snowfall Trends in Vermont\n\n\n\nClimate\n\n\nR\n\n\nStatistics\n\n\nTime Series\n\n\nMEDS\n\n\n\nA statistical time series analysis of snowfall and temperature between the years 2001 - 2022\n\n\n\nCarly Caswell\n\n\nDec 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Air Quality Impacts of Santa Barbara County’s Thomas Fire\n\n\n\nPython\n\n\nGIS\n\n\nClimate\n\n\nMEDS\n\n\n\nVisualizing changes in air quality index (AQI). A blog for EDS220: Working with Environmental Datasets\n\n\n\nCarly Caswell\n\n\nDec 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence\n\n\n\nAI\n\n\nEthics\n\n\nClimate\n\n\nMEDS\n\n\n\nA blog for EDS242: Ethics and Bias in Data Science\n\n\n\nCarly Caswell\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carly Caswell",
    "section": "",
    "text": "My name is Carly and welcome to my personal website. This site will give you an overview of my professional life, interests, and passions (both personal and work-related).\nWhat am I up to right now?\nI am currently a Master of Environmental Data Science student at the University of California Santa Barbara, studying to apply data science skills to real challenges facing our ever-changing climate.\n\n\n\nMaster in Environmental Data Science [Present] University of California, Santa Barbara\nBS in Business Administration [2018] University of Vermont\n\n\n\n\nEntrepreneurship Teaching Assistant [Fall 2023] UCSB\nSolutions Consultant [2021 - 2023] Smartsheet\nAI Solutions Consultant [2018 - 2021] Quinyx"
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Carly Caswell",
    "section": "",
    "text": "My name is Carly and welcome to my personal website. This site will give you an overview of my professional life, interests, and passions (both personal and work-related).\nWhat am I up to right now?\nI am currently a Master of Environmental Data Science student at the University of California Santa Barbara, studying to apply data science skills to real challenges facing our ever-changing climate.\n\n\n\nMaster in Environmental Data Science [Present] University of California, Santa Barbara\nBS in Business Administration [2018] University of Vermont\n\n\n\n\nEntrepreneurship Teaching Assistant [Fall 2023] UCSB\nSolutions Consultant [2021 - 2023] Smartsheet\nAI Solutions Consultant [2018 - 2021] Quinyx"
  },
  {
    "objectID": "posts/2023-12-10-stats-post/index.html",
    "href": "posts/2023-12-10-stats-post/index.html",
    "title": "Evaluating the Impact of Climate Change on Snowfall Trends in Vermont",
    "section": "",
    "text": "Project Repository Located Here"
  },
  {
    "objectID": "posts/2023-12-10-stats-post/index.html#about-this-project",
    "href": "posts/2023-12-10-stats-post/index.html#about-this-project",
    "title": "Evaluating the Impact of Climate Change on Snowfall Trends in Vermont",
    "section": "About This Project",
    "text": "About This Project\n“Since 2000, the number of days with snow cover globally has declined as Earths climate warms, and southern New England has lost nearly a month of its annual snow cover,”(Boston Globe). With climate change being a major contributor in rising temperatures across the globe, lack of snowfall has become a serious problem that researchers, scientists, economists, and industry experts are working to solve. Snowfall is an important climate change indicator because of it’s influence on the larger environmental community. Snowfall provides water sources in the Springtime for wildlife, the agriculture industry, and for human consumption. It is also a key contributor to the ski and tourism industry, which impacts the economy for some states that significantly rely on ski and snow tourism.\nVermont is one state that relies heavily on its winter tourism, so my aim was to analyze the changes in snowfall in this state. I have personally felt the impacts on lack of snow in recent years in this state, with limitations to skiing and ski resorts having to make more snow than ever before. My aim is to understand if there are seasonal temperature and snowfall differences in Vermont’s yearly snow patterns by comparing data from the last 22 years."
  },
  {
    "objectID": "posts/2023-12-10-stats-post/index.html#research-question",
    "href": "posts/2023-12-10-stats-post/index.html#research-question",
    "title": "Evaluating the Impact of Climate Change on Snowfall Trends in Vermont",
    "section": "Research Question",
    "text": "Research Question\nIs there a statistically significant difference between the recent versus older trends in average snowfall and temperature changes in Vermont?"
  },
  {
    "objectID": "posts/2023-12-10-stats-post/index.html#analysis-highlights",
    "href": "posts/2023-12-10-stats-post/index.html#analysis-highlights",
    "title": "Evaluating the Impact of Climate Change on Snowfall Trends in Vermont",
    "section": "Analysis Highlights:",
    "text": "Analysis Highlights:\n\nTidy and wrangle snowfall and temperature data\nVisualize my data\nCheck for normality\nRun two hypothesis tests\nComplete a regression analysis"
  },
  {
    "objectID": "posts/2023-12-10-stats-post/index.html#about-the-data",
    "href": "posts/2023-12-10-stats-post/index.html#about-the-data",
    "title": "Evaluating the Impact of Climate Change on Snowfall Trends in Vermont",
    "section": "About the Data:",
    "text": "About the Data:\nI gathered daily snow depth and temperature data from the USDA’s Natural Resources Conservation Service and National Water and Climate Center (https://nwcc-apps.sc.egov.usda.gov/site-plots/#VT). This data source contains two datasets:\nDataset 1 - Titled vt_temps , this dataset contains daily temperature data (in degrees Fahrenheit) from the years 2001 - 2023. These data points were collected from a weather station at the Mount Mansfield SCAN Site numbered 2041. Mount Mansfield is Vermont’s largest mountain peak, located in the Green Mountains at an elevation of 4395’.\nDataset 2 - Titled vt_snow , this dataset contains daily snowfall data (in inches) from the years 2001 - 2023. These data points were collected from a weather station at the Mount Mansfield SCAN Site numbered 2041. Mount Mansfield is Vermont’s largest mountain peak, located in the Green Mountains at an elevation of 4395’."
  },
  {
    "objectID": "posts/2023-12-10-stats-post/index.html#preliminary-data-analysis",
    "href": "posts/2023-12-10-stats-post/index.html#preliminary-data-analysis",
    "title": "Evaluating the Impact of Climate Change on Snowfall Trends in Vermont",
    "section": "Preliminary Data Analysis",
    "text": "Preliminary Data Analysis\nFor my data analysis, I completed a number of steps before getting to my final results. Those steps included cleaning, wrangling, visualizing, and analyzing the datasets that I had.\n\nReading in the Data\n\n\nCode\nlibrary(tidyverse) #for tidying my data\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\nlibrary(dplyr) #for wrangling\nlibrary(dotwhisker) #for pretty plotting\nlibrary(plotly) #for plotting\nlibrary(lubridate) #for extracting specific years/months in the data\nlibrary(sf)\nlibrary(ggmap)\nlibrary(tmap)\nlibrary(here)\nlibrary(zoo) #moving averages\n\n# Setting my filepath here:\nrootdir &lt;- (\"~/Documents/MEDS/Fall_Q2/EDS-222-Stats/Assignments\")\ndatadir &lt;- file.path(rootdir,\"eds222-final\",\"data\")\n\n\n\n\nCode\nvt_temps &lt;- read_csv(file.path(datadir,\"Mount_Mansfield_temps.csv\"))    \nvt_snow &lt;- read_csv(file.path(datadir,\"Mount_Mansfield_snow.csv\"))\n\n\n\n\nData Cleaning\nAfter reading in the data, I noticed both datasets were not tidy, so I completed a number of steps to tidy the data into a usable format.\n\nTemperature Data Updates\n\n\nCode\n#Making this data tidy using pivot_longer\ntidy_vt_temps &lt;- vt_temps %&gt;%\n  pivot_longer(cols = c('2001', '2002', '2003', '2004', '2005', '2006', '2007', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2021', '2022', '2023'), names_to = \"year\", values_to = \"value\") %&gt;% \n  select(-'2009', -'2010', -'2019', -'2024') \n\n#Creating a combined date column since year and date are currently separate\ntidy_vt_temps$date &lt;- paste(tidy_vt_temps$year, tidy_vt_temps$date, sep = \"-\")\nclass(tidy_vt_temps$date) #this column is still a character so this needs to be updated to a date\ntidy_vt_temps$date &lt;- ymd(tidy_vt_temps$date)\nclass(tidy_vt_temps$date) #nice, using lubridate worked to make a date column\n\n\n\n\nSnowfall Data Updates\n\n\nCode\n#Making this data tidy using pivot_longer\ntidy_vt_snow &lt;- vt_snow %&gt;%\n  pivot_longer(cols = c('2001', '2002', '2003', '2004', '2005', '2006', '2007', '2011', '2012', '2013', '2015', '2016', '2017', '2018', '2022', '2023'), names_to = \"year\", values_to = \"value\") %&gt;% \n  select(-'2009', -'2010', -'2020', -'2024') \n\n#Creating a combined date column since year and date are currently separate\ntidy_vt_snow$date &lt;- paste(tidy_vt_snow$year, tidy_vt_snow$date, sep = \"-\")\nclass(tidy_vt_snow$date) #it's still a character so this needs to be updated\ntidy_vt_snow$date &lt;- ymd(tidy_vt_snow$date)\nclass(tidy_vt_snow$date) #nice, it's now a date column\n\n\n\n\n\nData Wrangling\nAfter tidying the data, I wanted to take a look at it visually and narrow down the data I am going to work with:\n\nTemperature Data Updates\n\n\nCode\n#I have data for all months of the year\n#I've decided I want to compare just months November - December for this analysis\n#First need to create a month column:\ntidy_vt_temps_winter &lt;- tidy_vt_temps %&gt;% \n   mutate(month = month(date)) %&gt;% \n  rename(temp = value)\n\n#Now I can filter for only the months I want:\ntidy_vt_temp_winter_years &lt;-\ntidy_vt_temps_winter %&gt;% \n  filter(month %in% c(11,12)) %&gt;% \n  mutate(month_day = sprintf(\"%02d-%02d\", month(date), day(date)))\n\n\n\n\nSnowfall Data Updates\n\n\nCode\n#I have data for all months of the year\n#I've decided I want to compare just months November - December for this analysis\n#First need to create a month column:\ntidy_vt_snow_winter &lt;- tidy_vt_snow %&gt;% \n  mutate(month = month(tidy_vt_snow$date)) %&gt;% \n  rename(snowfall = value)\n\n#Now I can filter for only the months I want:\ntidy_vt_snow_winter_years &lt;- \ntidy_vt_snow_winter %&gt;% \n   filter(month %in% c(11,12)) %&gt;% \n  mutate(month_day = sprintf(\"%02d-%02d\", month(date), day(date))) #also making a new column for graphing purposes later\n\n\n\n\n\nData Visualization\nI’m now going to take a look at multiple years of snowfall and temperature data in order to have visual evidence of any key trends I can gather from November and December between 2001 and 2023.\n\n\n\nPictured: Comparing Snowfall on Mount Mansfield in November and December\n\n\n\n\n\nPictured: Comparing Temperature on Mount Mansfield in November and December\n\n\nDetails on the code used to create these plots can be found below:\n\n\nCode\n#Taking a look at daily snowfall from 2001 - 2023\nplot_ly(tidy_vt_snow_winter_years, \n        x = ~month_day, \n        y = ~snowfall, \n        color = ~as.factor(year), \n        type = \"scatter\",\n        mode = \"lines+markers\",\n        line = list(width = 1)) %&gt;%\n  layout(title = \"Comparison of Snowfall on Mount Mansfield from 2001 to 2022\",\n         xaxis = list(title = \"Dates\"),\n         yaxis = list(title = \"Snowfall (in inches)\"),\n         legend = list(title = \"Year\"))\n\n\n\n\nCode\n#Taking a look at daily temperature from 2001 - 2023\nplot_ly(tidy_vt_temp_winter_years, \n        x = ~month_day, \n        y = ~temp, \n        color = ~as.factor(year), \n        type = \"scatter\",\n        mode = \"lines+markers\",\n        line = list(width = 1)) %&gt;%\n  layout(title = \"Comparison of Temperature Recorded on Mount Mansfield from 2001 to 2022\",\n         xaxis = list(title = \"Dates\"),\n         yaxis = list(title = \"Temperature (in Fahrenheit)\"),\n         legend = list(title = \"Year\"))\n\n\nI noticed that for temperature (in Fahrenheit), recent years of lowest values (specifically 2021 and 2022) that have been recorded are significantly higher than the lows measured in non-recent years (specifically 2001 and 2002). I also noticed that the peaks for snowfall recorded (in inches) tend to be much higher in the non-recent years (specifically 2001 and 2002). These two generalizations indicates that there may indeed be some differences in snowfall and temperature over the duration of time I have plotted.\nI also noticed that there are a few years where data is missing, so I will omit those years from my analysis.\nWith these conclusions, I’m able to narrow down my focus to sample and group a number of years to compare in my hypothesis testing. To do this, I will categorize my years into “recent” and “non-recent” based on the data that is available.Given there was quite sparse data for many years, with some years not even being in the dataset, I was limited to working with the years that did have complete data. Interestingly, I could not find information on the organization’s website about why there were years missing or there were many NAs in the years that did have data.\nTherefore, for this analysis, I will indicate the “recent” grouping as years 2015, 2016, and 2022 since those years have the most data. The “non-recent” grouping I will indicate as years 2001, 2022, and 2003 due to completeness in the data. Time to do some statistical analyses!"
  },
  {
    "objectID": "posts/2023-12-10-stats-post/index.html#my-statistical-analyses",
    "href": "posts/2023-12-10-stats-post/index.html#my-statistical-analyses",
    "title": "Evaluating the Impact of Climate Change on Snowfall Trends in Vermont",
    "section": "My Statistical Analyses",
    "text": "My Statistical Analyses\nFor my question, I will consider differences in climate conditions (snowfall and temperature) in Vermont and conduct summary statistics, a simple hypothesis test, a linear regression model, and a decomposition. I am aiming to determine if there is a statistically significant difference between the recent and non-recent years for both snowfall and temperature variables.\n\nSummary Statistics\nBy bucketing my data into two groups, recent and non-recent (more information on why and how below), I can do an initial comparison of my two sample groups:\nSnowfall Table:\n\n\n\n\n\n\n\n\n\nYears\nMean (inches)\nStandard Deviation (inches)\nMedian (inches)\n\n\n\n\nRecent (2015,2016, 2022)\n3.92\n8.37\n0\n\n\nNon-Recent (2001, 2002, 2003)\n8.61\n11.48\n0\n\n\n\nTemperature Table:\n\n\n\n\n\n\n\n\n\nYears\nMean (F)\nStandard Deviation (F)\nMedian (F)\n\n\n\n\nRecent (2015,2016, 2022)\n42.28\n18.83\n44.06\n\n\nNon-Recent (2001, 2002, 2003)\n39.64\n19.43\n41.18\n\n\n\nWith these initial summary statistics, I can already see differences in the means, standard deviation, and median for my two groups. Taking a look at the snowfall data, the mean and standard deviation snowfall values (in inches) are much lower in the recent years grouping than non-recent. This aligns with my initial assumption that snowfall in recent years is, on average, much more sparse than previous years. When comparing the temperature statistics, I also noted mean temperature values (in Fahrenheit) to be much higher in the recent years grouping than the non-recent years. This also aligns with my initial assumption that temperatures are increasing. Given temperature and snowfall are highly correlated due to our understanding of snowfall’s reliance on the temperature threshold of 32 degrees Fahrenheit, it would make sense that with higher average temperature values the average snowfall would decrease since they are inversely related. Now that I have some initial summary statistics, I am going to run my hypothesis test!\nCode for calculating these values below:\n\n\nCode\n# I'm going to join my two datasets together first:\nvt_data &lt;- merge(tidy_vt_snow_winter, tidy_vt_temps_winter, by = 'date')\n\n#Cleaning my datasets\nvt_data &lt;- vt_data %&gt;% \n  select(-\"Min.x\", -\"10%.x\",-\"30%.x\", -\"70%.x\", -\"90%.x\", -\"Max.x\", -\"Median (POR).x\", -\"year.x\", -\"month.x\", -\"Min.y\",-\"10%.y\", -\"30%.y\",-\"70%.y\",-\"90%.y\",-\"Max.y\",-\"Median (POR).y\") %&gt;% \n  rename(year = \"year.y\", month = \"month.y\") %&gt;%  #removing and renaming columns to tidy this dataframe \n  filter( year %in% c('2001', '2002', '2003', '2015', '2016','2022'))\n\nvt_data\n\n#Summarizing the snowfall and temperature data for recent years:\nvt_recent &lt;- \n  vt_data %&gt;% \n  filter(year %in% c('2015', '2016', '2022'))\n\nvt_recent_stats &lt;- vt_recent %&gt;% \nsummarise(snow_mean = mean(snowfall, na.rm = TRUE), snow_sd = sd(snowfall, na.rm = TRUE), snow_median = median(snowfall, na.rm = TRUE),temp_mean = mean(temp, na.rm = TRUE), temp_sd = sd(temp, na.rm = TRUE), temp_median = median(temp, na.rm = TRUE))\n\n#visualizing the stats table:\nvt_recent_stats\n\n#Summarizing the snowfall and temperature data before for non-recent years:\nvt_non_recent &lt;- \n  vt_data %&gt;% \n  filter(year %in% c('2001', '2002', '2003'))\n\nvt_non_recent_stats &lt;- vt_non_recent %&gt;% \nsummarise(snow_mean = mean(snowfall, na.rm = TRUE), snow_sd = sd(snowfall, na.rm = TRUE), snow_median = median(snowfall, na.rm = TRUE),temp_mean = mean(temp, na.rm = TRUE), temp_sd = sd(temp, na.rm = TRUE), temp_median = median(temp, na.rm = TRUE))\n\n#visualizing the stats table:\nvt_non_recent_stats\n\n\n\n\nChecking OLS Assumptions\nI am aiming to draw conclusions about the Vermont snow climate using this sample from Mount Mansfield of temperature and snowfall observations. In order to move forward with my statistical analyses and draw conclusions, I need to check my OLS assumptions. To do that, I am going to plot the distribution of my snowfall and temperature samples to check for normality before continuing.\n\nTemperature Distribution\n\n\n\nPictured: Visualizing Temperature Distribution\n\n\n\n\n\nPictured: Visualizing Temperature QQ Plot\n\n\nThe temperature data is fairly symmetric and this distribution looks to be somewhat normally distributed, which means I’m not going to transform my data since it is relatively normal.\n\n\nCode\n#Temperature Distributions Check\n#Non recent years\nvt_hist_non &lt;-\n  ggplot(vt_non_recent) +\n  geom_histogram(aes(x = temp)) +\nlabs(\n    title = \"Recent Temperature Distribution During November and December\",\n    x = \"Temperature (in Fahrenheit)\",\n    y = \"Frequency\"\n  )\nvt_hist_non\n\n\n\n\n\nCode\n#Recent years\nvt_hist_recent &lt;-\n  ggplot(vt_non_recent) +\n  geom_histogram(aes(x = temp)) +\nlabs(\n    title = \"Recent Temperature Distribution During November and December\",\n    x = \"Temperature (in Fahrenheit)\",\n    y = \"Frequency\"\n  )\nvt_hist_recent\n\n\n\n\n\n\n\nCode\n#QQ Plot for Temperature\nggplot(vt_data, aes(sample = temp)) +\n  geom_qq() +\n  geom_qq_line() +\n  ggtitle(\"Q-Q Plot for Temperature\")\n\n\n\n\n\n\n\nSnowfall Distribution\n\n\n\nPictured: Visualizing Snowfall Distribution\n\n\n\n\n\nPictured: Transformed Snowfall QQ Plot\n\n\nInteresting! The snowfall data was is fairly skewed to the right and there is a large right tail. This makes sense given the sheer volume of zeros in the data. Given this does not show me a normal distribution, I transformed the data using log() and created a QQ plot to see how how this non-normal distribution fits. Although my QQ plot is not perfect, I am happy with moving forward given the number of low values in my dataset (0s and 1s) which makes sense contextually since we are dealing with daily snowfall.\n\n\nCode\n#Creating histograms of my snowfall distribution to check for normality:\n#Snowfall Distributions Check\n#Non recent years\nvt_hist_non &lt;-\n  ggplot(vt_non_recent) +\n  geom_histogram(aes(x = snowfall)) +\nlabs(\n    title = \"Recent Snowfall Distribution During November and December\",\n    x = \"Snowfall (inches)\",\n    y = \"Frequency\"\n  )\nvt_hist_non\n\n\n\n\n\nCode\n#Recent years\nvt_hist_recent &lt;-\n  ggplot(vt_non_recent) +\n  geom_histogram(aes(x = snowfall)) +\nlabs(\n    title = \"Recent Snowfall Distribution During November and December\",\n    x = \"Snowfall (inches)\",\n    y = \"Frequency\"\n  )\nvt_hist_recent\n\n\n\n\n\n\n\nCode\n#Both recent and non-recent look relatively similar in their underlying distributions, so I'm going to plot just recent data.\n##QQ Plot for Snowfall\nggplot(vt_recent, aes(sample = snowfall)) +\n  geom_qq() +\n  geom_qq_line() +\n  ggtitle(\"Q-Q Plot for Snowfall\")\n\n\n\n\n\n\n\nCode\n##Transformed QQ Plot for Snowfall\nlog_snowfall &lt;-log(vt_recent$snowfall)\nlog_snowfall_non &lt;- log(vt_non_recent$snowfall)\n\nggplot(vt_recent, aes(sample = log_snowfall)) +\n  geom_qq() +\n  geom_qq_line() +\n  ggtitle(\"Q-Q Plot for Transformed Precipitation Data\")\n\n\n\n\n\nNow that I have checked for normality and updated data where it was needed, I’m going to go ahead and run my hypothesis tests.\n\n\n\nHypothesis Testing\n\nSnowfall Hypothesis\nIs the mean snowfall in inches different in recent years than in previous years?\nMy null and alternative hypotheses:\n\\[H_{O}: \\mu_{recent} - \\mu_{nonrecent} = 0\\]\n\\[H_{A}: \\mu_{recent} - \\mu_{nonrecent} \\neq 0\\]\n\n\nCode\n#Categorizing data\nvt_data$category &lt;- ifelse(vt_data$year %in% c(2015, 2016, 2022), \"recent\", \"not\")\n\n#Running a t.test on the filtered data:\nresult &lt;- t.test(snowfall ~ category, data = vt_data, conf.level = 0.95)\nprint(result)\n\n#Extracting my confidence interval\nconf_interval &lt;- result$conf.int\nprint(paste('I am 95% confident the range', round(conf_interval[1], 2), 'and',  round(conf_interval[2], 2), 'contains the true population difference between recent and non recent years for average snowfall in inches on Mount Mansfield.'))\n\n\n\n\n\nPictured: Welch’s Two Sample T Test Results\n\n\nTo conclude, my Welch’s two sample t test generates a p value of .000000058 with a significance level of .05. In this case, I would reject the null hypothesis and this result is statistically significant at the .05 significance level. I have strong evidence to suggest that the observed difference between snowfall in recent years and non-recent years is unlikely to have occurred by random chance alone.\nConsiderations for other variables that could influence this analysis includes wind speeds at the weather station, el nino/la nina years, and the extraction of N/As. Given this analysis was completed using one weather station for duration of 20 years, I’d like to acknowledge that changes could have occurred to the monitor’s position, measurement variability, and quality since snowfall is a quite sensitive variable.\n\n\nTemperature Hypothesis\nIs the mean temperature in Fahrenheit different in recent years than in previous years?\nMy null and alternative hypotheses:\n\\[H_{O}: \\mu_{recent} - \\mu_{nonrecent} = 0\\]\n\\[H_{A}: \\mu_{recent} - \\mu_{nonrecent} \\neq 0\\]\n\n\nCode\n#Categorizing data\nvt_data$category &lt;- ifelse(vt_data$year %in% c(2015, 2016, 2022), \"recent\", \"not\")\n\n#Running a t.test on the filtered data:\nresult &lt;- t.test(temp ~ category, data = vt_data, conf.level = 0.95)\nprint(result)\n\n#Extract confidence interval\nconf_interval &lt;- result$conf.int\nprint(paste('I am 95% confident the range', round(conf_interval[1], 2), 'and',  round(conf_interval[2], 2), 'contains the true population difference between recent and non recent years for average temperature in F on Mount Mansfield.'))\n\n\n To conclude, my Welch’s two sample t test generates a p value of .3706 with a significance level of .05. In this case, I would fail to reject the null hypothesis with this result not being statistically significant at the .05 significance level. I do not have strong evidence to suggest that the observed difference between snowfall in recent years and non-recent years is unlikely to have occurred by random chance alone.\nConsiderations for other variables that could influence this analysis includes percipitation at the weather station, el nino/la nina years, relative humidity, and the extraction of N/As. Given this analysis was completed using one weather station for duration of 20 years, I’d like to acknowledge that changes could have occurred to the monitor’s position, measurement variability, and quality since snowfall is a quite sensitive variable.\n\n\n\nLinear Regression\nI decided to expand my dataset again and include all years to compare the relationship between snowfall and temperature. Knowing that this relationship should be fairly intuitive since snowfall relies on a temperature threshold to exist (proven scientifically), I wanted to see the relationship that is present in this data.\n\n\n\nPictured: Running a Linear Regression with Results\n\n\nIntercept: When the average temperature is 0 degrees Fahrenheit, the snowfall is, on average, 20.6 inches between 2001-2023.\nSlope: For every 1 degree increase in temperature each day, the rate of snowfall decreases by .35.\nR-Squared: With an r-squared value of .40, I can conclude that 40% of snowfall in inches can be explained by temperature alone.\nCode for this analysis below:\n\n\nCode\n# I'm going to join my larger datasets together first:\nvt_all_years_data &lt;- merge(tidy_vt_snow_winter, tidy_vt_temps_winter, by = 'date')\n\n#taking a look at the data:\nvt_all_years_data &lt;- vt_all_years_data %&gt;% \n  select(-\"Min.x\", -\"10%.x\",-\"30%.x\", -\"70%.x\", -\"90%.x\", -\"Max.x\", -\"Median (POR).x\", -\"year.x\", -\"month.x\", -\"Min.y\",-\"10%.y\", -\"30%.y\",-\"70%.y\",-\"90%.y\",-\"Max.y\",-\"Median (POR).y\") %&gt;% \n  rename(year = \"year.y\", month = \"month.y\")\nvt_all_years_data\n#aggregating all years of data for snowfall and temp:\n\n#running a simple regression on snowfall and temperature variables for my sampling years\nsummary(lm(snowfall ~ temp, data = vt_all_years_data))"
  },
  {
    "objectID": "posts/2023-12-10-stats-post/index.html#conclusion-future-considerations",
    "href": "posts/2023-12-10-stats-post/index.html#conclusion-future-considerations",
    "title": "Evaluating the Impact of Climate Change on Snowfall Trends in Vermont",
    "section": "Conclusion & Future Considerations",
    "text": "Conclusion & Future Considerations\nTo conclude, and as shown above, there is no evidence to conclude statistically significant changes in average temperature data on Mount Mansfield from the years 2001 - 2023. There is evidence to suggest statistically significant changes in average snowfall in inches on Mount Mansfield from 2001 - 2023. However, as I also outlined above, there are a number of considerations with this data that could limit making any conclusions, including other variables present when gathering this data and missing years of data.\nThese conclusions align with my initial presumption that snowfall was declining in Vermont through my experiences skiing in recent years and also aligns with recent news stating the winter tourism industry is suffering from lack of snow. This analysis could be helpful for future ski industry stakeholders in determining how to budget and maintain mountains in the future. This analysis could also be helpful in determining mitigation plans for areas where snowfall is a significant contributor to spring water sources for agriculture, wildlife, and humans. It’s important we acknowledge these changes to our environment and consider what the future of our world may look like with less snowfall.\nIn a future analysis, this data would be helpful in doing the following:\n\nTime series forecasting to predict future snowfall patterns\nEvent analysis by understanding specific events in the time series like massive storms, El Niño/La Niña considerations \nA seasonal decomposition with a larger time frame of data\nExpanding the analysis regionally, nationally, or globally with more data"
  },
  {
    "objectID": "posts/2023-12-10-stats-post/index.html#sources",
    "href": "posts/2023-12-10-stats-post/index.html#sources",
    "title": "Evaluating the Impact of Climate Change on Snowfall Trends in Vermont",
    "section": "Sources:",
    "text": "Sources:\nThe Boston Globe. (2023, August 3). Climate change and winter: What the future holds. https://www.bostonglobe.com/2023/08/03/science/climate-change-winter/\nU.S. Department of Agriculture, Natural Resources Conservation Service. (n.d.). NWCC Site Plots - Vermont. USDA. https://nwcc-apps.sc.egov.usda.gov/site-plots/#VT"
  },
  {
    "objectID": "posts/2023-12-06 python/eds220-finalblog.html",
    "href": "posts/2023-12-06 python/eds220-finalblog.html",
    "title": "Understanding Air Quality Impacts of Santa Barbara County’s Thomas Fire",
    "section": "",
    "text": "Project Repository Located Here With More Details"
  },
  {
    "objectID": "posts/2023-12-06 python/eds220-finalblog.html#about-this-project",
    "href": "posts/2023-12-06 python/eds220-finalblog.html#about-this-project",
    "title": "Understanding Air Quality Impacts of Santa Barbara County’s Thomas Fire",
    "section": "About This Project",
    "text": "About This Project\nThe Thomas Fire was a wildfire that occured in December 2017 in Santa Barbara County and it had devastating impacts on the community. Knowing the risks, dangers, and effects of wildfires on populated areas, my analysis aims to visualize data surrounding key aspects of wildfires: the damage to the physical area and the air quality. In order to to do this, I will create a false color image to understand the burn area of the Thomas fire in 2017 as well as create a visualization to show the impact of air quality in Santa Barbara during the time period of the fire.\n\nPictured: Thousands of Acres Burn Across Santa Barbara County. Sourced by Al Seib of the Los Angeles Times\nClick here for more details on the Thomas Fire\n\nAnalysis Highlights:\n-Geospatial data exploration using pandas\n-Data wrangling and manipulation of raster and tabular data\n-Data analysis to calculate moving averages for air quality index during the period of the Thomas Fire (2017)\n-Creating and customizing a map of the Thomas Fire burn area and AQI moving averages from 2017-2018\n\n\nAbout the Data:\nDataset 1: Landsat Data\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. For more information on Landsat bands, reference: (https://www.usgs.gov/faqs/what-are-band-designations-landsat-satellites)\nClick here for the data source\nDataset 2: CA Fires\nA shapefile of fire perimeters in California during 2017.\nClick here for the data source\nDataset 3: AQI\nNational Air Quality Index (AQI) data from the US Environmental Protection Agency. I am specifically going to analyze AQI data for 2017 and 2018 in Santa Barbara County.\nClick here for the data source"
  },
  {
    "objectID": "posts/2023-12-06 python/eds220-finalblog.html#calculating-the-rolling-averages",
    "href": "posts/2023-12-06 python/eds220-finalblog.html#calculating-the-rolling-averages",
    "title": "Understanding Air Quality Impacts of Santa Barbara County’s Thomas Fire",
    "section": "Calculating the Rolling Averages",
    "text": "Calculating the Rolling Averages\nWe want to evaluate the change in air quality from before, during, and after the fire in 2017 and 2018. To do this, we need to calculate a 5 day rolling average of the air quality index data. To do this, we first need to make sure we are only looking at Santa Barbara County data, combine our 2017 and 2018 data, and clean up any fields necessary. Then we can calculate moving averages and plot the data to evaluate any impacts of the Thomas Fire on Santa Barbara County.\n\n\nCode\n#Calculating rolling window for the AQI_SB data:\naqi_sb.aqi.rolling('5D').mean()\n\n# Adding a new column with the 5-day rolling mean:\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()\n\n\nNow we can plot the rolling averages!\n\n\nCode\n#Plotting the AQI 5-day average:\nplt.figure(figsize=(12, 6))\nplt.plot(aqi_sb.index, aqi_sb['aqi'], label='Daily AQI', color='cornflowerblue')\nplt.plot(aqi_sb.index, aqi_sb['five_day_average'], label='5-Day Average', color='salmon')\nplt.title('AQI 5-Day Average for Santa Barbara County')\nplt.xlabel('Date')\nplt.ylabel('AQI')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2023-12-06 python/eds220-finalblog.html#creating-a-map-of-ca-thomas-fire",
    "href": "posts/2023-12-06 python/eds220-finalblog.html#creating-a-map-of-ca-thomas-fire",
    "title": "Understanding Air Quality Impacts of Santa Barbara County’s Thomas Fire",
    "section": "Creating a Map of CA Thomas Fire",
    "text": "Creating a Map of CA Thomas Fire\nWe also want to understand the impacts of the Thomas Fire on Santa Barbara County through mapping. To do this, we are going to create a false color image of Santa Barbara County with the landsat bands data and map this with our Thomas fire perimeter to understand where in California the fire spanned and how large it was.\n\n\nCode\n#Creating a false color image \nbands[[\"swir22\", \"nir08\", \"red\"]].to_array()\n#Now we have our image we can graph this with our Thomas Fire perimeter\n\n\n\n\nCode\n## Plotting the shapefile\nfig, ax = plt.subplots()\nsize = 6 #height in of plot \naspect = bands.rio.width/bands.rio.height\nfig.set_size_inches(size, size*aspect) #why? bc cannot use ax and size aspect together\nbands[[\"swir22\", \"nir08\", \"red\"]].to_array().plot.imshow(ax=ax, robust = True)\nca_fires_new .plot(ax=ax,facecolor='none', edgecolor='red', linewidth=2, alpha=0.5)\n\n## Set plot title\nplt.title('Map of the Thomas Fire Perimeter (2017) in California')\n\n# Remove the axes\nplt.axis('off')\n\n# Set legend with linestyle='None'\nlegend_elements = [mlines.Line2D([0], [0], color='red', marker = '_', linestyle='None', markersize=10, label='Thomas Fire')]\n\n# Add legend\nax.legend(handles=legend_elements)\n\n## Showing the plot\nplt.show()"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html",
    "href": "posts/2023-12-08-geospatial-post/index.html",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "",
    "text": "Project Repository Located Here"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#about-this-project",
    "href": "posts/2023-12-08-geospatial-post/index.html#about-this-project",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "About This Project",
    "text": "About This Project\n“As the human population looks set to reach 10 billion people by 2050, our food systems will be under intense pressure to produce animal protein for an increasing population,”(Gentry). This is an important problem to consider because, with scarcity of animal-rich protein sources, we need to turn to other alternatives. This is where marine aquaculture can play an important role, which is defined as the breeding, rearing, and harvesting of aquatic plants and animals (NOAA). Sustainable seafood production can be a signfiicant contributor to this global food supply crisis, and in order to plan for future marine aquaculture projects and initiatives, we need to consider a few important implications, such as ocean depth, temperatures, logistics traffic, protected areas, etc.\nIn this analysis, given a species input, I am going to analyze what the determined Exclusive Economic Zones (EEZ) are on the West Coast of the United States that would be best suited to developing marine aquaculture for that species of interest.\n\n\n\nPictured: The US Exclusive Economic Zones (EEZ). Sourced by the National Oceanic and Atmospheric Association\n\n\nClick here for more details on the EEZ Zones"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#analysis-highlights",
    "href": "posts/2023-12-08-geospatial-post/index.html#analysis-highlights",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Analysis Highlights:",
    "text": "Analysis Highlights:\n\nCombining vector and raster data\nResampling raster data using resample function\nMasking raster data using raster package\nOperations on raster data\nCreating interactive maps using leaflet\nDesigning a function for any species"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#about-the-data",
    "href": "posts/2023-12-08-geospatial-post/index.html#about-the-data",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "About the Data:",
    "text": "About the Data:\nWhen considering suitability for marine aquaculture of a specific species, we need to consider two primary conditions: Depth and Temperature. When finding suitable EEZ regions, we are going to need data on the depth and surface temperature details (datasets 1 and 2). We can then find suitable regions using the Exclusive Economic Zones (dataset 3), which are specific zones that the United States and coastal nations have rights and jurisdiction over the resources in those waters. More details can be found in the shared link above.\nDataset 1: Sea Surface Temperature\nThis data is average annual sea surface temperature (SST) from the years 2008 to 2012. This will be used to characterize the average sea surface temperature within the EEZ region.To find the averages for each year, I created a raster stack of each year titled sst (sea surface temperature).\nThis data was generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1\nDataset 2: Bathymetry\nThis data is raster data characterizing the depth of the ocean, defined as a raster titled depth.\nThis data was generated from General Bathymetric Chart of the Oceans (GEBCO).\nDataset 3: Exclusive Economic Zones\nTo narrow the focus for this analysis, I will just be focusing on West Coast US Exclusive Economic Zones.\nThis data was used as a shapefile titled wc_eez.\nThis data was generated from Marineregions.org."
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#final-results",
    "href": "posts/2023-12-08-geospatial-post/index.html#final-results",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Final Results",
    "text": "Final Results\nI created a function (outlined below) that utilizes the following inputs:\nspecies_name: your species of interest. Note: this should be a realistic species for human commercial consumption.\nmin_temp: minimum suitable temperature in Celsius, found on SeaLifeBase\nmax_temp: maximum suitable temperature in Celsius, found on SeaLifeBase\nmin_depth: minimum meters below sea level for optimal growth, found on SeaLifeBase\nmax_depth: maximum meters below sea level for optimal growth, found on SeaLifeBase\n\nspecies_function = function(species_name, min_temp, max_temp, min_depth, max_depth){\n\n# read in the shapefile for the West Coast EEZ\nwc_eez &lt;- st_read(here('./data2/wc_regions_clean.shp'))\n\n# read in your SST rasters\nfilelist &lt;- list.files(\"./data2/average_annual/\", full.names = TRUE)\nfilelist \n\n# Make SST rasters into a raster stack\nsst &lt;- rast(filelist)\n\n# Read in bathymetry raster (`depth.tif`)\ndepth &lt;- rast(here(\"data2\", \"depth.tif\"))\n\n# reproject any data not in the same projection\nsst &lt;- project(sst, \"EPSG:4326\")\n#st_crs(wc_eez) == st_crs(sst)\n#st_crs(depth) == st_crs(sst)\n\n#Finding the mean SST\nsst_mean &lt;- app(sst, fun = \"mean\")\n#Converting SST from kelvin to Celsius\nsst_mean_cel &lt;- sst_mean - 273.15\n\n#Cropping depth to match extent of SST \nnew_depth &lt;- crop(depth, sst_mean_cel)\n\n#Resampling the depth data to match SST using nearest neighbor\nnew_depth_resample &lt;- resample(new_depth, y = sst, method = \"near\")\n\n#Reclassifying for sea surface temperature depending on species suitable conditions \nrcl_sst &lt;- matrix(c(-Inf, min_temp, NA,\n                min_temp,max_temp, 1,\n                max_temp, Inf, NA),\n              ncol = 3, byrow = TRUE)\n\n#reclassifing the sst raster\nsst_temp_rcl &lt;- classify(sst_mean_cel, rcl = rcl_sst)\n\n\n#Reclassifying for sea level depth depending on species suitable conditions \n#Assumption: sea level is 0 so anything below sea level is negative\nmax_depth &lt;- -(max_depth) #converting max depth to negative, representative of this assumption\n\nrcl_d &lt;- matrix(c(max_depth,min_depth,1,\n                      -Inf, max_depth, NA,\n                      min_depth,Inf,NA),\n                    ncol = 3, byrow = TRUE)\n\n#reclassifying sea depth level raster \nsst_depth_rcl &lt;- classify(new_depth_resample, rcl = rcl_d)\n\n#Finding locations that satisfy both SST and depth conditions\nsuitable_locations &lt;- lapp(c(sst_depth_rcl, sst_temp_rcl), \"*\")\n\n\n#Next finding suitable cells within WC_EEZ:\nsuitable_regions &lt;- crop(suitable_locations, wc_eez)\nsuitable_regions &lt;- mask(suitable_regions, wc_eez)\n\n#Finding the total area of grid cells:\narea_grid &lt;- expanse(suitable_regions)\n\n#Finding total suitable area within EEZ:\neez_raster &lt;- rasterize(wc_eez, suitable_regions, field = \"rgn\")\n\n#Adding the area percent as another component to suitable area:\nsuitable_area &lt;- cross_join(wc_eez, area_grid) %&gt;% \n  mutate(area_perc = (area/ area_m2)* 100 )\n\n#Creating map for total suitable area by region\nmap1 &lt;- \n  tm_shape(suitable_area) +\n  tm_fill(\"area_km2\",palette = (c(\"#90e0ef\", \"#48cae4\", \"#00b4d8\", \n                          \"#0096c7\", \"#0077b6\", \"#023e8a\")), title = \"Area of Suitable Habitat (m^2) by EEZ Region\") +  tm_layout(\"Suitable Area for Oysters by EEZ Region\", legend.outside = TRUE) +\n   tm_borders(col = \"gray\") +\n  tm_basemap(\"OpenStreetMap\") \n  \nprint(map1)  \n#Creating map for percentage of total suitable area by region\n\nmap2 &lt;-\n  tm_shape(suitable_area) +\n  tm_fill(\"area_perc\",palette = \"Purples\", title = \"Percentage of Suitable Area by EEZ Region\") +\n  tm_layout(\"Suitable Percentage of Area for Oysters by EEZ Region\", legend.outside = TRUE) +\n   tm_borders(col = \"gray\") +\n  tm_basemap(\"OpenStreetMap\")\n\nprint(map2)\n}\n\nThe outputs of the function create two maps showcasing the total suitable area for the species’ growth as well as the percentage (%) of suitable area for the species’ growth.\nIn the examples below, this output is showing results for species of oysters:"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#conclusion",
    "href": "posts/2023-12-08-geospatial-post/index.html#conclusion",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Conclusion",
    "text": "Conclusion\nTo conclude, I am able to indicate that the largest area of suitable habitat for oysters in the EEZ region is in Central California. The percentage of suitable area indicates the highest percentage of suitable area is in the Washington region of the West Coast EEZ. This data is helpful for guiding actions or policies on sustainable marine aquaculture for oyster species, indicating those projects should occur in those regions for the best results.\nIn a future analysis, I would aim to look at existing population data of oysters or other species in this area to help drive decisions on likely size, requirements, and budgets for a marine aquaculture project on oyster species."
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#details-of-the-data-analysis",
    "href": "posts/2023-12-08-geospatial-post/index.html#details-of-the-data-analysis",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Details of the Data Analysis",
    "text": "Details of the Data Analysis\nFor my data analysis, I completed a number of steps before getting to mapped suitable areas. These steps included cleaning, wrangling, plotting, and manipulating the datasets that I had.\n\nFirst, I needed to do some basic data cleaning. This included re-projecting my data to make sure all was using the same coordinate reference system to accurately compare and layer maps.\nSecond, I calculated the mean sea surface temperature over the raster stacks and converted it to Celsius. This provided me with a raster of means for the entire West Coast. In order to just look at our Exclusive Economic Zones, I used crop to reduce the raster data to our area of interest. Then, because the resolution of the data did not match, I also used the nearest neighbor method using resample to make sure the resolutions matched and checked that the coordinate reference systems were the same.\nThird, I plotted the data. In the visualization below, I can see the variation in average sea surface temperatures in Celsius for the West Coast EEZ.\n\n\n\n\nSea Surface Temperature in Celsius\n\n\n\nFourth, I needed to reclassify the data (using reclassify) to meet both depth and sea temperature specifications for the species of interest. As an example in this analysis, I will use oysters, however the function created as part of this analysis will represent any species of choosing. Given I wanted to understand suitable oyster regions, I found information on species depth and temperature requirements on SeaLifeBase.This was used to find information on oyster’s suitable sea surface temperature and depth specifications which was determined to be between 11 and 30 degrees Celsius and an optimal depth of 0-70 meters below sea level.\nFifth, I wanted to determine the total suitable area within each EEZ in order to rank specific zones/regions by priority. To do so, I found the total area of suitable locations within each EEZ using crop, mask, expanse, and rasterize. I could then find the total suitable area within each EEZ region, as seen from the plot below: \nLastly, I cross-joined the data for suitable area by region on to the EEZ data to plot them in the same map and calculated the percentage of area using the total grid area of the West Coast EEZ (calculated using expanse).\n\nAfter all of the six steps in the analysis, I had the data in order to plot my final results (as seen above)."
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#sources",
    "href": "posts/2023-12-08-geospatial-post/index.html#sources",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Sources:",
    "text": "Sources:\nGentry, R.R., Froehlich, H.E., Grimm, D. et al. Mapping the global potential for marine aquaculture. Nat Ecol Evol 1, 1317–1324 (2017). https://doi.org/10.1038/s41559-017-0257-9\nNational Oceanic and Atmospheric Administration (NOAA). Understanding Marine Aquaculture. Retrieved from https://www.fisheries.noaa.gov/insight/understanding-marine-aquaculture#:~:text=Marine%20aquaculture%20refers%20to%20the,salmon%2C%20and%20other%20marine%20fish.\nNational Oceanic and Atmospheric Administration. (NOAA). 2023. Exclusive Economic Zone (EEZ). Retrieved from https://oceanservice.noaa.gov/facts/eez.html"
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html",
    "href": "posts/2023-12-06 python/eds242-finalblog.html",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "",
    "text": "Artificial Intelligence (AI) stands out as a prevailing buzzword of this era. It is everywhere we live, work, and interact - from the advertisements on our screens to the technology in our homes and in integral services like those provided by banks, airports, and hospitals. At the basis of AI is algorithms, a systematic set of instructions or rules used to solve problems. Focusing on efforts related to climate work, algorithm application can be applied to predicting temperature changes, weather events, future deforestation, and carbon emissions. It can be used to show the effects of extreme weather, potential benefits of carbon capture and regenerative agriculture, and even nudge the general public to pursue climate-friendly ways of changing habits and behaviors (Coeckelbergh).\nAlgorithms, however, can lead to biased decision-making. There is plenty of evidence that exists to prove biases in algorithms, including research on algorithms detecting skin cancer that was only effective on light skin tones because of a non-demographically diverse dataset (Calderon) or Amazon’s hiring algorithm exhibiting gender bias when designed to review job applicants, favoring male candidates over female candidates because the training data reflected a male-dominated workforce (Dastin). Algorithmic-generated content needs to be critiqued and reviewed through the lens of auditing. Algorithmic auditing is a crucial way to address the challenges associated with the increasing use of algorithms and dependence of them on our decision-making. In order to audit algorithms effectively, we need to question the basis of AI before allowing it to drive our human-based decisions (whether we consider AI to be “moral” decision-making is a whole other topic that I won’t get into today). When considering environmentally-focused algorithmic decision-making, I think it’s crucial to contemplate three aspects of algorithm creation: Data, Documentation, and Decision-makers (3Ds)."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#the-three-ds-analyzing-the-implications-of-data-documentation-and-decision-making-in-climate-driven-artificial-intelligence",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#the-three-ds-analyzing-the-implications-of-data-documentation-and-decision-making-in-climate-driven-artificial-intelligence",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "",
    "text": "Artificial Intelligence (AI) stands out as a prevailing buzzword of this era. It is everywhere we live, work, and interact - from the advertisements on our screens to the technology in our homes and in integral services like those provided by banks, airports, and hospitals. At the basis of AI is algorithms, a systematic set of instructions or rules used to solve problems. Focusing on efforts related to climate work, algorithm application can be applied to predicting temperature changes, weather events, future deforestation, and carbon emissions. It can be used to show the effects of extreme weather, potential benefits of carbon capture and regenerative agriculture, and even nudge the general public to pursue climate-friendly ways of changing habits and behaviors (Coeckelbergh).\nAlgorithms, however, can lead to biased decision-making. There is plenty of evidence that exists to prove biases in algorithms, including research on algorithms detecting skin cancer that was only effective on light skin tones because of a non-demographically diverse dataset (Calderon) or Amazon’s hiring algorithm exhibiting gender bias when designed to review job applicants, favoring male candidates over female candidates because the training data reflected a male-dominated workforce (Dastin). Algorithmic-generated content needs to be critiqued and reviewed through the lens of auditing. Algorithmic auditing is a crucial way to address the challenges associated with the increasing use of algorithms and dependence of them on our decision-making. In order to audit algorithms effectively, we need to question the basis of AI before allowing it to drive our human-based decisions (whether we consider AI to be “moral” decision-making is a whole other topic that I won’t get into today). When considering environmentally-focused algorithmic decision-making, I think it’s crucial to contemplate three aspects of algorithm creation: Data, Documentation, and Decision-makers (3Ds)."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#data-weve-got-a-problem",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#data-weve-got-a-problem",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Data, We’ve Got a Problem",
    "text": "Data, We’ve Got a Problem\nAs a Data Scientist, I interact with new and evolving datasets just about every day. This data is sourced from a variety of topics and contributors, and in the work that I do, I’ve been taught to take a deeper look, ask questions, and consider the biases that went into the data. According to Jennifer Logg, an Assistant Professor of Management at Georgetown University, “….algorithms can efficiently compound bias that is present in the input data. An algorithm will magnify any patterns in the input data, so if bias is present, the algorithm will also magnify that bias, “(Rock). Climate justice relies heavily on accurate data representation to inform policies and decision-making processes. When biased data becomes the inputs to algorithms that shape these decisions, it can result in disproportionate impacts on marginalized communities. If these biases are not properly addressed during algorithmic analysis, it can lead to the reinforcement of existing disparities. For example, an algorithm that relies on biased pollution data might misallocate resources in its decision-making, leaving already vulnerable communities without protection. To achieve environmental justice in AI-driven climate work, we need to acknowledge, scrutinize, and correct biases in the data used by algorithms. By prioritizing fairness in data analysis, we can build algorithms that contribute to equitable environmental decisions, policies, and practices."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#document-document-document",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#document-document-document",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Document! Document! Document!",
    "text": "Document! Document! Document!\nSimilar to the creation of datasheets for datasets, algorithm-driven decisions need to have documentation formulating the inner workings of the algorithm, including its design, logic, and decision-making processes. This transparency can allow auditors and stakeholders to understand how the algorithm operates, which is essential for assessing its fairness, accuracy, and potential biases. I believe documentation in the form of some sort of watermarking system, could allow users to have transparency and future trust in the algorithms they are interacting with. Thorough documentation, in general, contributes to better transparency, reproducibility, and accountability of algorithmic systems, making the auditing process more effective and reliable. With watermarking systems, we can ensure that auditors have “checked” off the necessary information for assessing an algorithm’s performance, and if unable to watermark for approval, could identify potential issues, and make informed recommendations for improvement."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#who-decides",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#who-decides",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Who Decides?",
    "text": "Who Decides?\nEffective auditing (as well as creation) of algorithms is reliant on informed decision-makers who understand the nuances of ethical data and decision-making. As Coecklebergh states in AI for Climate, “….those who develop and use AI have a special (in the sense of “specific”) responsibility. To make sure that AI leads to a greener and more climate friendly world is definitely also the responsibility of computer scientists, engineers, designers, managers, investors, and others involved in, managing, and promoting, AI and data science practices,” (Coeckelbergh). We also need to bring to the table decision-makers that are going to make fair decisions and keep the general public in mind. Consider Google’s recent creation of their Advanced Technology Ethics Advisory council, which aimed to advise on the company’s usage of AI. “….they were not transparent about their roles, responsibilities, and authority. Rather than engage affected communities, Google appointed a Council member who opposed LGBT rights. Google’s approach to oversight fostered distrust and protests, and the Council was dissolved,” (Calderon). In my opinion, human intervention will always be needed to create checks and balances with any form of AI that is driving our decisions, behaviors, and analyses. In the realm of climate justice, where algorithmic systems can impact something like policy formation, knowledgeable, fair, and adequately represented decision-makers are crucial to the formation and usage of AI in climate work."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#ai-dont-worry-we-still-love-you",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#ai-dont-worry-we-still-love-you",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "AI, Don’t Worry We Still Love You",
    "text": "AI, Don’t Worry We Still Love You\nTo conclude, in order to promote ethical data usage and responsible AI application we must safeguard future use of AI in environmental justice by taking a fine-tooth comb to the 3Ds (Data, Documentation, and Decision-makers). As Jennifer Logg so eloquently stated, “Trashing the mirror does not heal the bruise, but it could prolong the time it takes to fix the problem and detect future ones,”(Rock). In an era where algorithms increasingly influence critical aspects of our lives, of indigenous communities, of nature, and of our planet, we need to ensure these rapidly emerging systems undergo rigorous scrutiny and auditing. If we implement the three D’s to algorithm creation and usage, and even consider a “stamp of approval”, we can have some level of a “digital signature”, attesting to the legitimacy and ethical compliance of the underlying processes. This stamp of approval could then be implemented in policies and compliance for future creation and usage of AI in not just climate application, but any field."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#sources",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#sources",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Sources:",
    "text": "Sources:\nCalderon, A., Taber, D., Qu, H., Wen, J., et al. (2019). AI Blindspot Cards. Retrieved from www.aiblindspot.com (Version 1.1).\nCoeckelbergh, M. (2020). “AI for climate: freedom, justice, and other ethical and political challenges.” AI and Ethics, Pg 1-6.\nDastin, J. (2018, October 10). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. Retrieved from https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G\nRock, D., Whittlestone, J., & Garrett, N. (2019, August 7). Using Algorithms to Understand the Biases in Your Organization. Harvard Business Review. Retrieved from https://hbr.org/2019/08/using-algorithms-to-understand-the-biases-in-your-organization\n​​Smith, J. (2022, March 15). How AI Can Help Tackle Climate Change. Techopedia, https://www.techopedia.com/how-ai-can-help-tackle-climate-change/2/33622"
  },
  {
    "objectID": "posts/2024-02-28-viz-post/index.html",
    "href": "posts/2024-02-28-viz-post/index.html",
    "title": "Utilizing Strava Data to Evalute My Marathon Training",
    "section": "",
    "text": "Project Repository Located Here"
  },
  {
    "objectID": "posts/2024-02-28-viz-post/index.html#my-motivating-questions",
    "href": "posts/2024-02-28-viz-post/index.html#my-motivating-questions",
    "title": "Utilizing Strava Data to Evalute My Marathon Training",
    "section": "My Motivating Questions",
    "text": "My Motivating Questions\nMy infographic (seen below) is a representation of my Strava data during marathon training. I started on this journey because I wanted to look back at my last marathon training and asked myself: How should I train for my next marathon?! Luckily, I had recorded all of my running data with the public online media platform called Strava. Strava has allowed me to keep all of my data on running and cross-training, which has been amazing to keep track of and learn about as I go. By analyzing my Strava data, some more detailed questions I will pursue are: When in my training should I ramp up my weekly mileage? What days during the week should I expect longer distances/need to allocate more time to running on those days? And lastly, what types of activities should I do to cross-train? I like to stay active beyond running, and wanted to look back at some of the other activities I’d done to balance getting miles in without road running."
  },
  {
    "objectID": "posts/2024-02-28-viz-post/index.html#about-the-data",
    "href": "posts/2024-02-28-viz-post/index.html#about-the-data",
    "title": "Utilizing Strava Data to Evalute My Marathon Training",
    "section": "About the Data",
    "text": "About the Data\nThis data comes from my personal Strava data documented on the Strava web application. This application is a social network for tracking physical exercise via activities. Each logged data point represents an activity I have done; these activities range from running, hiking, surfing, skiing, walking, strength training, etc. In order to get this data from the public Strava app, I referenced the Strava API . This includes utilizing the {{rStrava}} package. This package allowed me to scrape my data from the public Strava website by creating a Strava application and using authentication. Details on the scraped activity data included where I completed the activity, the duration of the activity, the type of activity, the title of the activity, the distance I went, the day/month/year I did the activity, the number of kudos received, and essentially anything recorded in a single Strava activity that I have access to. Then, I filtered my activity data down to my time period of interest, which was May to October 2023 (the time I was training for the marathon)."
  },
  {
    "objectID": "posts/2024-02-28-viz-post/index.html#the-approach",
    "href": "posts/2024-02-28-viz-post/index.html#the-approach",
    "title": "Utilizing Strava Data to Evalute My Marathon Training",
    "section": "The Approach",
    "text": "The Approach\nTBD"
  },
  {
    "objectID": "posts/2024-02-28-viz-post/index.html#preliminary-data-analysis",
    "href": "posts/2024-02-28-viz-post/index.html#preliminary-data-analysis",
    "title": "Utilizing Strava Data to Evalute My Marathon Training",
    "section": "Preliminary Data Analysis",
    "text": "Preliminary Data Analysis\n\nReading in the Data\n\n\nCode\n#Loading Libraries ---\nlibrary(rStrava)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(paletteer) #for color palettes\nlibrary(patchwork)\nlibrary(magick)\nlibrary(ggpubr)\nlibrary(png)\n\n\n\n\nCode\n#Reading in Data ---\n#strava_activities &lt;- read_csv(\"strava_activities.csv\")\n\n\n\n\nData Wrangling"
  },
  {
    "objectID": "posts/2024-02-28-viz-post/index.html#final-result",
    "href": "posts/2024-02-28-viz-post/index.html#final-result",
    "title": "Utilizing Strava Data to Evalute My Marathon Training",
    "section": "Final Result",
    "text": "Final Result\n\nVisualization 1\n\n\nVisualization 2\n\n\nVisualization 3"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Carly Caswell",
    "section": "",
    "text": "--- title: “Projects”\n---"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Professionally\nMy previous experience involves helping large organizations work more efficiently with software as a service tools.\nAfter graduating with an undergraduate degree in Business Analytics, my goal was to work with data and gain experience understanding how businesses operate. I spent three years working as an Artificial Intelligence Solutions Consultant, which included designing and creating solutions to implement algorithms into workforce management tools. I helped Fortune 500 companies (examples included McDonald’s,DoorDash, & Nike) optimize the costs associated with forecasting their demand, staffing labor, and allocating resources effectively.\nWith an interest in expanding my skills and working for a larger organization, I then became a Consultant for Smartsheet, a collaborative work management tool. At Smartsheet, I worked as a hybrid management and IT consultant analyzing client’s existing processes and recommending and designing scalable solutions for them. I managed large project budgets, cross-functional work teams, and multi-year solution implementations.\n\n\n\nOnsite with a Client in Chicago\n\n\nAfter being in the workforce for five years, I began to re-think my career goals and found myself wanting to do more impactful work. Having experienced climate change first-hand and being passionate about saving our only planet, I realized I needed to pivot careers to focus on the environment. With my love for working with data and code from my time as an undergrad, I decided to go pursue my Master’s degree to transition to this new line of work, combining both my love for data with my interest in climate solutions. My goal with this degree is, by combining my previous consulting and technology experience and my new knowledge of data science, I can provide helpful data science approaches to large organizations with a sustainable mission.\n\n\n Personally\nIn my free time, I love to be outdoors with my dog, Kona. I am grateful to live in California where the weather allows me to be outside year round. My hobbies include trail running, hiking, yoga, surfing, skiing, and tennis. Growing up in Vermont, I loved skiing and hiking with family and friends. I now exploring the Sierras, skiing, hiking, and finding new trails wherever I go.\n\n\n\nTrail Running in the Sawtooths with Kona\n\n\nI grew up being a sprint runner, competing in cross country races across New England. In the last two years I have recently explored long distance running races, partaking in numerous half marathons and the Lake Tahoe Marathon. My next goal is to qualify for and run the 2025 Boston Marathon.\n\n\n\nFinishing the 2023 Lake Tahoe Marathon"
  }
]
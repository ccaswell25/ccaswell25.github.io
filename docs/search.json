[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "More details to come!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "More details to come!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Carly Caswell",
    "section": "",
    "text": "--- title: “Projects”\n---"
  },
  {
    "objectID": "posts/2023-12-06 python/eds220-finalblog.html",
    "href": "posts/2023-12-06 python/eds220-finalblog.html",
    "title": "Understanding Air Quality Impacts of Santa Barbara County’s Thomas Fire",
    "section": "",
    "text": "Project Repository Located Here With More Details"
  },
  {
    "objectID": "posts/2023-12-06 python/eds220-finalblog.html#about-this-project",
    "href": "posts/2023-12-06 python/eds220-finalblog.html#about-this-project",
    "title": "Understanding Air Quality Impacts of Santa Barbara County’s Thomas Fire",
    "section": "About This Project",
    "text": "About This Project\nThe Thomas Fire was a wildfire that occured in December 2017 in Santa Barbara County and it had devastating impacts on the community. Knowing the risks, dangers, and effects of wildfires on populated areas, my analysis aims to visualize data surrounding key aspects of wildfires: the damage to the physical area and the air quality. In order to to do this, I will create a false color image to understand the burn area of the Thomas fire in 2017 as well as create a visualization to show the impact of air quality in Santa Barbara during the time period of the fire.\n\nPictured: Thousands of Acres Burn Across Santa Barbara County. Sourced by Al Seib of the Los Angeles Times\nClick here for more details on the Thomas Fire\n\nAnalysis Highlights:\n-Geospatial data exploration using pandas\n-Data wrangling and manipulation of raster and tabular data\n-Data analysis to calculate moving averages for air quality index during the period of the Thomas Fire (2017)\n-Creating and customizing a map of the Thomas Fire burn area and AQI moving averages from 2017-2018\n\n\nAbout the Data:\nDataset 1: Landsat Data\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. For more information on Landsat bands, reference: (https://www.usgs.gov/faqs/what-are-band-designations-landsat-satellites)\nClick here for the data source\nDataset 2: CA Fires\nA shapefile of fire perimeters in California during 2017.\nClick here for the data source\nDataset 3: AQI\nNational Air Quality Index (AQI) data from the US Environmental Protection Agency. I am specifically going to analyze AQI data for 2017 and 2018 in Santa Barbara County.\nClick here for the data source"
  },
  {
    "objectID": "posts/2023-12-06 python/eds220-finalblog.html#calculating-the-rolling-averages",
    "href": "posts/2023-12-06 python/eds220-finalblog.html#calculating-the-rolling-averages",
    "title": "Understanding Air Quality Impacts of Santa Barbara County’s Thomas Fire",
    "section": "Calculating the Rolling Averages",
    "text": "Calculating the Rolling Averages\nWe want to evaluate the change in air quality from before, during, and after the fire in 2017 and 2018. To do this, we need to calculate a 5 day rolling average of the air quality index data. To do this, we first need to make sure we are only looking at Santa Barbara County data, combine our 2017 and 2018 data, and clean up any fields necessary. Then we can calculate moving averages and plot the data to evaluate any impacts of the Thomas Fire on Santa Barbara County.\n\n\nCode\n#Calculating rolling window for the AQI_SB data:\naqi_sb.aqi.rolling('5D').mean()\n\n# Adding a new column with the 5-day rolling mean:\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()\n\n\nNow we can plot the rolling averages!\n\n\nCode\n#Plotting the AQI 5-day average:\nplt.figure(figsize=(12, 6))\nplt.plot(aqi_sb.index, aqi_sb['aqi'], label='Daily AQI', color='cornflowerblue')\nplt.plot(aqi_sb.index, aqi_sb['five_day_average'], label='5-Day Average', color='salmon')\nplt.title('AQI and 5-Day Average')\nplt.xlabel('Date')\nplt.ylabel('AQI')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2023-12-06 python/eds220-finalblog.html#creating-a-map-of-ca-thomas-fire",
    "href": "posts/2023-12-06 python/eds220-finalblog.html#creating-a-map-of-ca-thomas-fire",
    "title": "Understanding Air Quality Impacts of Santa Barbara County’s Thomas Fire",
    "section": "Creating a Map of CA Thomas Fire",
    "text": "Creating a Map of CA Thomas Fire\nWe also want to understand the impacts of the Thomas Fire on Santa Barbara County through mapping. To do this, we are going to create a false color image of Santa Barbara County with the landsat bands data and map this with our Thomas fire perimeter to understand where in California the fire spanned and how large it was.\n\n\nCode\n#Creating a false color image \nbands[[\"swir22\", \"nir08\", \"red\"]].to_array()\n#Now we have our image we can graph this with our Thomas Fire perimeter\n\n\n\n\nCode\n## Plotting the shapefile\nfig, ax = plt.subplots()\nsize = 6 #height in of plot \naspect = bands.rio.width/bands.rio.height\nfig.set_size_inches(size, size*aspect) #why? bc cannot use ax and size aspect together\nbands[[\"swir22\", \"nir08\", \"red\"]].to_array().plot.imshow(ax=ax, robust = True)\nca_fires_new .plot(ax=ax,facecolor='none', edgecolor='red', linewidth=2, alpha=0.5)\n\n## Set plot title\nplt.title('Map of the Thomas Fire Perimeter (2017) in California')\n\n# Remove the axes\nplt.axis('off')\n\n# Set legend with linestyle='None'\nlegend_elements = [mlines.Line2D([0], [0], color='red', marker = '_', linestyle='None', markersize=10, label='Thomas Fire')]\n\n# Add legend\nax.legend(handles=legend_elements)\n\n## Showing the plot\nplt.show()"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html",
    "href": "posts/2023-12-08-geospatial-post/index.html",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "",
    "text": "Project Repository Located Here"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#about-this-project",
    "href": "posts/2023-12-08-geospatial-post/index.html#about-this-project",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "About This Project",
    "text": "About This Project\nA surprising fact (cited). This is an important problem to consider because\nThis is where marine aquaculture can play an important role, which is defined as….\nWith the global food supply crisis, sustainable protein alternatives to meat production have proven to be sufficiently important in the future of our food supply. In order to consider planning for future marine aquaculture projects and initiatives, we need to consider a few important implications, such as ocean depth, temperatures, logistics traffic, etc.\nIn this analysis, given a species input, I am going to analyze what the determined Exclusive Economic Zones (EEZ) are on the West Coast of the United States that would be best suited to developing marine aquaculture for that species of interest.\n\n\n\nPictured: The US Exclusive Economic Zones (EEZ). Sourced by the National Oceanic and Atmospheric Association\n\n\nClick here for more details on the EEZ Zones"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#analysis-highlights",
    "href": "posts/2023-12-08-geospatial-post/index.html#analysis-highlights",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Analysis Highlights:",
    "text": "Analysis Highlights:\n\nCombining vector and raster data\nResampling raster data using resample function\nMasking raster data using raster package\nOperations on raster data\nCreating interactive maps using leaflet\nDesigning a function for any species"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#about-the-data",
    "href": "posts/2023-12-08-geospatial-post/index.html#about-the-data",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "About the Data:",
    "text": "About the Data:\nWhen considering suitability for marine aquaculture for a specific species, we need to consider two primary conditions: Depth and Temperature. When finding suitable EEZ regions, we are going to need data on the depth and surface temperature details (datasets 1 and 2). We can then find suitable regions using the Exclusive Economic Zones (EEZ, dataset 3), which are…..\nDataset 1: Sea Surface Temperature\nThis data is average annual sea surface temperature (SST) from the years 2008 to 2012. This will be used to characterize the average sea surface temperature within the EEZ region.To find the averages for each year, I created a raster stack of each year titled sst (sea surface temperature).\nThis data was generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1\nDataset 2: Bathymetry\nThis data is raster data characterizing the depth of the ocean, defined as a raster titled depth.\nThis data was generated from General Bathymetric Chart of the Oceans (GEBCO).\nDataset 3: Exclusive Economic Zones\nTo narrow the focus for this analysis, I will just be focusing on West Coast US Exclusive Economic Zones. These zones serve as ………\nThis data was used as a shapefile titled wc_eez.\nThis data was generated from Marineregions.org."
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#final-results",
    "href": "posts/2023-12-08-geospatial-post/index.html#final-results",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Final Results",
    "text": "Final Results\nI created a function (outlined below) that utilizes the following inputs:\nspecies_name: your species of interest. Note: this should be a realistic species for human commerical consumption.\nmin_temp: minimum suitable temperature in Celsius, found on SeaLifeBase\nmax_temp: maximum suitable temperature in Celsius, found on SeaLifeBase\nmin_depth: minimum meters below sea level for optimal growth, found on SeaLifeBase\nmax_depth: maximum meters below sea level for optimal growth, found on SeaLifeBase\n….figure out how to hide this output so it’s a dropdown in the blog….\n\nspecies_function = function(species_name, min_temp, max_temp, min_depth, max_depth){\n\n# read in the shapefile for the West Coast EEZ\nwc_eez &lt;- st_read(here('./data2/wc_regions_clean.shp'))\n\n# read in your SST rasters\nfilelist &lt;- list.files(\"./data2/average_annual/\", full.names = TRUE)\nfilelist \n\n# Make SST rasters into a raster stack\nsst &lt;- rast(filelist)\n\n# Read in bathymetry raster (`depth.tif`)\ndepth &lt;- rast(here(\"data2\", \"depth.tif\"))\n\n# reproject any data not in the same projection\nsst &lt;- project(sst, \"EPSG:4326\")\nst_crs(wc_eez) == st_crs(sst)\nst_crs(depth) == st_crs(sst)\n\n#Finding the mean SST\nsst_mean &lt;- app(sst, fun = \"mean\")\n#Converting SST from kelvin to Celsius\nsst_mean_cel &lt;- sst_mean - 273.15\n\n#Cropping depth to match extent of SST \nnew_depth &lt;- crop(depth, sst_mean_cel)\n\n#Resampling the depth data to match SST using nearest neighbor\nnew_depth_resample &lt;- resample(new_depth, y = sst, method = \"near\")\n\n#Reclassifying for sea surface temperature depending on species suitable conditions \nrcl_sst &lt;- matrix(c(-Inf, min_temp, NA,\n                min_temp,max_temp, 1,\n                max_temp, Inf, NA),\n              ncol = 3, byrow = TRUE)\n\n#reclassifing the sst raster\nsst_temp_rcl &lt;- classify(sst_mean_cel, rcl = rcl_sst)\n\n\n#Reclassifying for sea level depth depending on species suitable conditions \n#Assumption: sea level is 0 so anything below sea level is negative\nmax_depth &lt;- -(max_depth) #converting max depth to negative, representative of this assumption\n\nrcl_d &lt;- matrix(c(max_depth,min_depth,1,\n                      -Inf, max_depth, NA,\n                      min_depth,Inf,NA),\n                    ncol = 3, byrow = TRUE)\n\n#reclassifying sea depth level raster \nsst_depth_rcl &lt;- classify(new_depth_resample, rcl = rcl_d)\n\n#Finding locations that satisfy both SST and depth conditions\nsuitable_locations &lt;- lapp(c(sst_depth_rcl, sst_temp_rcl), \"*\")\n\n\n#Next finding suitable cells within WC_EEZ:\nsuitable_regions &lt;- crop(suitable_locations, wc_eez)\nsuitable_regions &lt;- mask(suitable_regions, wc_eez)\n\n#Finding the total area of grid cells:\narea_grid &lt;- expanse(suitable_regions)\n\n#Finding total suitable area within EEZ:\neez_raster &lt;- rasterize(wc_eez, suitable_regions, field = \"rgn\")\n\n#Adding the area percent as another component to suitable area:\nsuitable_area &lt;- cross_join(wc_eez, area_grid) %&gt;% \n  mutate(area_perc = (area/ area_m2)* 100 )\n\n#Creating map for total suitable area by region\nmap1 &lt;- \n  tm_shape(suitable_area) +\n  tm_fill(\"area_km2\",palette = (c(\"#90e0ef\", \"#48cae4\", \"#00b4d8\", \n                          \"#0096c7\", \"#0077b6\", \"#023e8a\")), title = \"Area of Suitable Habitat (km^2) by EEZ Region\") +  tm_layout(\"Suitable Area for Oysters by EEZ Region\", legend.outside = TRUE) +\n   tm_borders(col = \"gray\") +\n  tm_basemap(\"OpenStreetMap\") \n  \nprint(map1)  \n#Creating map for percentage of total suitable area by region\n\nmap2 &lt;-\n  tm_shape(suitable_area) +\n  tm_fill(\"area_perc\",palette = \"Purples\", title = \"Percentage of Suitable Area by EEZ Region\") +\n  tm_layout(\"Suitable Percentage of Area for Oysters by EEZ Region\", legend.outside = TRUE) +\n   tm_borders(col = \"gray\") +\n  tm_basemap(\"OpenStreetMap\")\n\nprint(map2)\n}\n\nThe outputs of the function create two maps showcasing the total suitable area for the species’ growth as well as the percentage (%) of suitable area for the species’ growth.\nIn the examples below, this output is showing results for species of oysters:"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#conclusion",
    "href": "posts/2023-12-08-geospatial-post/index.html#conclusion",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Conclusion",
    "text": "Conclusion\nTo conclude, I am able to indicate…..add results and conclusions. Discussion of conclusions, including any potential caveats or future directions"
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#details-of-the-data-analysis",
    "href": "posts/2023-12-08-geospatial-post/index.html#details-of-the-data-analysis",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Details of the Data Analysis",
    "text": "Details of the Data Analysis\nFor my data analysis, I had to take a number of steps to getting complete suitable areas mapped.\n\nfirst needed to do some basic data cleaning such as reprojecting to make sure all fields were using the same coordinate reference system (so we can accurately compare and layer maps).\n\nI then took the mean sea surface temperature over the raster stacks and converted it to Celsius.This provided me with a raster of means for the entire West Coast. In order to just look at our Exclusive Economic Zones, I used crop to reduce the ratser data to our area of interest. Because the resolution of the data do not match, I also the nearest neighbor method using resample to make sure the resolutions match and checked the coordinate reference systems were the same.\nIn the visualization below, I can see the variation in average sea surface temperatures in Celsius for the West Coast EEZ.\n\n\n\nSea Surface Temperature in Celsius\n\n\nNext, I need to reclassify the data (using reclassify) to meet both depth and sea temperature specifications for the species of interest. As an example in this analysis, I will use oysters, however the function created as part of this analysis will showcase any species of choosing.\nGiven I want to understand suitable oyster regions, I found information on species depth and temperature requirements on SeaLifeBase. This was used to find information on oyster’s suitable sea surface temperature and depth specifications which was determined to be between 11 and 30 degrees celsius and optimal depth is 0-70 meters below sea level.\nThen I wanted to determine the total suitable area within each EEZ in order to rank specific zones/regions by priority. To do so, I found the total area of suitable locations within each EEZ using crop mask expanse and rasterize\nI found the total suitable area within each EEZ region, as seen from the plot below: \nThen I cross-joined the data for suitable area by region on to the EEZ data and calculated the percentage of area using the total grid area calculated using expanse\nAfter all of that analysis, I have the data I need to plot, as indicated in the results section (above)."
  },
  {
    "objectID": "posts/2023-12-08-geospatial-post/index.html#sources",
    "href": "posts/2023-12-08-geospatial-post/index.html#sources",
    "title": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States",
    "section": "Sources:",
    "text": "Sources:"
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html",
    "href": "posts/2023-12-06 python/eds242-finalblog.html",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "",
    "text": "Artificial Intelligence (AI) stands out as a prevailing buzzword of this era. It is everywhere we live, work, and interact - from the advertisements on our screens to the technology in our homes and in integral services like those provided by banks, airports, and hospitals. At the basis of AI is algorithms, a systematic set of instructions or rules used to solve problems. Focusing on efforts related to climate work, algorithm application can be applied to predicting temperature changes, weather events, future deforestation, and carbon emissions. It can be used to show the effects of extreme weather, potential benefits of carbon capture and regenerative agriculture, and even nudge the general public to pursue climate-friendly ways of changing habits and behaviors (Coeckelbergh).\nAlgorithms, however, can lead to biased decision-making. There is plenty of evidence that exists to prove biases in algorithms, including research on algorithms detecting skin cancer that was only effective on light skin tones because of a non-demographically diverse dataset (Calderon) or Amazon’s hiring algorithm exhibiting gender bias when designed to review job applicants, favoring male candidates over female candidates because the training data reflected a male-dominated workforce (Dastin). Algorithmic-generated content needs to be critiqued and reviewed through the lens of auditing. Algorithmic auditing is a crucial way to address the challenges associated with the increasing use of algorithms and dependence of them on our decision-making. In order to audit algorithms effectively, we need to question the basis of AI before allowing it to drive our human-based decisions (whether we consider AI to be “moral” decision-making is a whole other topic that I won’t get into today). When considering environmentally-focused algorithmic decision-making, I think it’s crucial to contemplate three aspects of algorithm creation: Data, Documentation, and Decision-makers (3Ds)."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#the-three-ds-analyzing-the-implications-of-data-documentation-and-decision-making-in-climate-driven-artificial-intelligence",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#the-three-ds-analyzing-the-implications-of-data-documentation-and-decision-making-in-climate-driven-artificial-intelligence",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "",
    "text": "Artificial Intelligence (AI) stands out as a prevailing buzzword of this era. It is everywhere we live, work, and interact - from the advertisements on our screens to the technology in our homes and in integral services like those provided by banks, airports, and hospitals. At the basis of AI is algorithms, a systematic set of instructions or rules used to solve problems. Focusing on efforts related to climate work, algorithm application can be applied to predicting temperature changes, weather events, future deforestation, and carbon emissions. It can be used to show the effects of extreme weather, potential benefits of carbon capture and regenerative agriculture, and even nudge the general public to pursue climate-friendly ways of changing habits and behaviors (Coeckelbergh).\nAlgorithms, however, can lead to biased decision-making. There is plenty of evidence that exists to prove biases in algorithms, including research on algorithms detecting skin cancer that was only effective on light skin tones because of a non-demographically diverse dataset (Calderon) or Amazon’s hiring algorithm exhibiting gender bias when designed to review job applicants, favoring male candidates over female candidates because the training data reflected a male-dominated workforce (Dastin). Algorithmic-generated content needs to be critiqued and reviewed through the lens of auditing. Algorithmic auditing is a crucial way to address the challenges associated with the increasing use of algorithms and dependence of them on our decision-making. In order to audit algorithms effectively, we need to question the basis of AI before allowing it to drive our human-based decisions (whether we consider AI to be “moral” decision-making is a whole other topic that I won’t get into today). When considering environmentally-focused algorithmic decision-making, I think it’s crucial to contemplate three aspects of algorithm creation: Data, Documentation, and Decision-makers (3Ds)."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#data-weve-got-a-problem",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#data-weve-got-a-problem",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Data, We’ve Got a Problem",
    "text": "Data, We’ve Got a Problem\nAs a Data Scientist, I interact with new and evolving datasets just about every day. This data is sourced from a variety of topics and contributors, and in the work that I do, I’ve been taught to take a deeper look, ask questions, and consider the biases that went into the data. According to Jennifer Logg, an Assistant Professor of Management at Georgetown University, “….algorithms can efficiently compound bias that is present in the input data. An algorithm will magnify any patterns in the input data, so if bias is present, the algorithm will also magnify that bias, “(Rock). Climate justice relies heavily on accurate data representation to inform policies and decision-making processes. When biased data becomes the inputs to algorithms that shape these decisions, it can result in disproportionate impacts on marginalized communities. If these biases are not properly addressed during algorithmic analysis, it can lead to the reinforcement of existing disparities. For example, an algorithm that relies on biased pollution data might misallocate resources in its decision-making, leaving already vulnerable communities without protection. To achieve environmental justice in AI-driven climate work, we need to acknowledge, scrutinize, and correct biases in the data used by algorithms. By prioritizing fairness in data analysis, we can build algorithms that contribute to equitable environmental decisions, policies, and practices."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#document-document-document",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#document-document-document",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Document! Document! Document!",
    "text": "Document! Document! Document!\nSimilar to the creation of datasheets for datasets, algorithm-driven decisions need to have documentation formulating the inner workings of the algorithm, including its design, logic, and decision-making processes. This transparency can allow auditors and stakeholders to understand how the algorithm operates, which is essential for assessing its fairness, accuracy, and potential biases. I believe documentation in the form of some sort of watermarking system, could allow users to have transparency and future trust in the algorithms they are interacting with. Thorough documentation, in general, contributes to better transparency, reproducibility, and accountability of algorithmic systems, making the auditing process more effective and reliable. With watermarking systems, we can ensure that auditors have “checked” off the necessary information for assessing an algorithm’s performance, and if unable to watermark for approval, could identify potential issues, and make informed recommendations for improvement."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#who-decides",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#who-decides",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Who Decides?",
    "text": "Who Decides?\nEffective auditing (as well as creation) of algorithms is reliant on informed decision-makers who understand the nuances of ethical data and decision-making. As Coecklebergh states in AI for Climate, “….those who develop and use AI have a special (in the sense of “specific”) responsibility. To make sure that AI leads to a greener and more climate friendly world is definitely also the responsibility of computer scientists, engineers, designers, managers, investors, and others involved in, managing, and promoting, AI and data science practices,” (Coeckelbergh). We also need to bring to the table decision-makers that are going to make fair decisions and keep the general public in mind. Consider Google’s recent creation of their Advanced Technology Ethics Advisory council, which aimed to advise on the company’s usage of AI. “….they were not transparent about their roles, responsibilities, and authority. Rather than engage affected communities, Google appointed a Council member who opposed LGBT rights. Google’s approach to oversight fostered distrust and protests, and the Council was dissolved,” (Calderon). In my opinion, human intervention will always be needed to create checks and balances with any form of AI that is driving our decisions, behaviors, and analyses. In the realm of climate justice, where algorithmic systems can impact something like policy formation, knowledgeable, fair, and adequately represented decision-makers are crucial to the formation and usage of AI in climate work."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#ai-dont-worry-we-still-love-you",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#ai-dont-worry-we-still-love-you",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "AI, Don’t Worry We Still Love You",
    "text": "AI, Don’t Worry We Still Love You\nTo conclude, in order to promote ethical data usage and responsible AI application we must safeguard future use of AI in environmental justice by taking a fine-tooth comb to the 3Ds (Data, Documentation, and Decision-makers). As Jennifer Logg so eloquently stated, “Trashing the mirror does not heal the bruise, but it could prolong the time it takes to fix the problem and detect future ones,”(Rock). In an era where algorithms increasingly influence critical aspects of our lives, of indigenous communities, of nature, and of our planet, we need to ensure these rapidly emerging systems undergo rigorous scrutiny and auditing. If we implement the three D’s to algorithm creation and usage, and even consider a “stamp of approval”, we can have some level of a “digital signature”, attesting to the legitimacy and ethical compliance of the underlying processes. This stamp of approval could then be implemented in policies and compliance for future creation and usage of AI in not just climate application, but any field."
  },
  {
    "objectID": "posts/2023-12-06 python/eds242-finalblog.html#sources",
    "href": "posts/2023-12-06 python/eds242-finalblog.html#sources",
    "title": "Analyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence",
    "section": "Sources:",
    "text": "Sources:\nCalderon, A., Taber, D., Qu, H., Wen, J., et al. (2019). AI Blindspot Cards. Retrieved from www.aiblindspot.com (Version 1.1).\nCoeckelbergh, M. (2020). “AI for climate: freedom, justice, and other ethical and political challenges.” AI and Ethics, Pg 1-6.\nDastin, J. (2018, October 10). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. Retrieved from https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G\nRock, D., Whittlestone, J., & Garrett, N. (2019, August 7). Using Algorithms to Understand the Biases in Your Organization. Harvard Business Review. Retrieved from https://hbr.org/2019/08/using-algorithms-to-understand-the-biases-in-your-organization\n​​Smith, J. (2022, March 15). How AI Can Help Tackle Climate Change. Techopedia, https://www.techopedia.com/how-ai-can-help-tackle-climate-change/2/33622"
  },
  {
    "objectID": "posts/2023-10-29-short-post-description/index.html",
    "href": "posts/2023-10-29-short-post-description/index.html",
    "title": "test blog post title",
    "section": "",
    "text": "I am going to insert a footnote here1."
  },
  {
    "objectID": "posts/2023-10-29-short-post-description/index.html#footnotes",
    "href": "posts/2023-10-29-short-post-description/index.html#footnotes",
    "title": "test blog post title",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is the associated footnote, which will appear at the bottom of my document in a “Footnotes” section.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carly Caswell",
    "section": "",
    "text": "My name is Carly and welcome to my personal website. This site will give you an overview of my professional life, interests, and passions (both personal and work-related).\nWhat am I up to right now?\nI am currently a Master of Environmental Data Science student at the University of California Santa Barbara, studying to apply data science skills to real challenges facing our ever-changing climate.\n\n\nMaster in Environmental Data Science [Present] University of California, Santa Barbara\nBS in Business Administration [2018] University of Vermont\n\n\n\nEntrepreneurship Teaching Assistant [2023 - Present] UCSB\nSolutions Consultant [2021 - 2023] Smartsheet\nAI Solutions Consultant [2018 - 2021] Quinyx"
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Carly Caswell",
    "section": "",
    "text": "My name is Carly and welcome to my personal website. This site will give you an overview of my professional life, interests, and passions (both personal and work-related).\nWhat am I up to right now?\nI am currently a Master of Environmental Data Science student at the University of California Santa Barbara, studying to apply data science skills to real challenges facing our ever-changing climate.\n\n\nMaster in Environmental Data Science [Present] University of California, Santa Barbara\nBS in Business Administration [2018] University of Vermont\n\n\n\nEntrepreneurship Teaching Assistant [2023 - Present] UCSB\nSolutions Consultant [2021 - 2023] Smartsheet\nAI Solutions Consultant [2018 - 2021] Quinyx"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Modeling Suitable Growth Areas for Marine Species on the West Coast of the United States\n\n\n\n[GIS, R, MEDS]\n\n\n\nDetermining growth suitability for developing marine aquaculture on the West Coast the United States Exclusive Economic Zones (EEZ).\n\n\n\nCarly Caswell\n\n\nDec 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Air Quality Impacts of Santa Barbara County’s Thomas Fire\n\n\n\nPython\n\n\nGIS\n\n\nClimate\n\n\nMEDS\n\n\n\nVisualizing changes in air quality index (AQI). A blog for EDS220: Working with Environmental Datasets\n\n\n\nCarly Caswell\n\n\nDec 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Implications of Data, Documentation, and Decision-Making in Climate-Driven Artificial Intelligence\n\n\n\nAI\n\n\nEthics\n\n\nClimate\n\n\nMEDS\n\n\n\nA blog for EDS242: Ethics and Bias in Data Science\n\n\n\nCarly Caswell\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest blog post title\n\n\n\nQuarto\n\n\nR\n\n\nMEDS\n\n\n\nblog post test description\n\n\n\nCarly Caswell\n\n\nOct 29, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]